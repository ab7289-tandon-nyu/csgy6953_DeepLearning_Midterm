{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ab7289-tandon-nyu/csgy6953_DeepLearning_Midterm/blob/oscar2/notebooks/oscar_play_osca_kernel_size_2_avgpool_replicate_resnet18_adamw_1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HqUIrHzYpEF"
      },
      "source": [
        "# Clone, Install Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD8cBXZ_YpEJ"
      },
      "source": [
        "## Timestamp this run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4MnxcGBFYpEL",
        "outputId": "3de000ac-8773-4521-aaab-5dce795bca2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (2022.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfovHN-sl04P",
        "outputId": "57a44886-e8bb-436a-bb13-3dda4931386e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/20(Sun)_10:36:05\n"
          ]
        }
      ],
      "source": [
        "# reference: https://www.programiz.com/python-programming/datetime/current-time\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "print(datetime.now(pytz.timezone('America/New_York')).strftime('%m/%d(%a)_%H:%M:%S'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEg6gGl4gdFv"
      },
      "source": [
        "# Wandb install, login, import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrgALvYPgMBy",
        "outputId": "cdd135cd-6db3-4fd9-bb27-04b629dfa216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.5)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBNArx2fgf-v",
        "outputId": "fca7dc63-c421-459f-bfb0-e3cade64625d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login \"6f19b1e6735ebc69af24f18d5b426262416027fb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MP8Tcl1ogjVP"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFOQTotOdDmI"
      },
      "source": [
        "## Torchsummary Install, Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "293u_6mjdMDE",
        "outputId": "fcf40f8e-0476-4eee-c468-141c8083e089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-summary==1.4.5 in /usr/local/lib/python3.7/dist-packages (1.4.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-summary==1.4.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8mq4tcWdOSP",
        "outputId": "d70a9780-3028-4fe0-eb50-b5b576c073c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torchsummary.torchsummary.summary(model: torch.nn.modules.module.Module, input_data: Union[torch.Tensor, torch.Size, Sequence[torch.Tensor], Sequence[Union[int, Sequence[Any], torch.Size]], NoneType] = None, *args: Any, batch_dim: Union[int, NoneType] = 0, branching: bool = True, col_names: Union[Iterable[str], NoneType] = None, col_width: int = 25, depth: int = 3, device: Union[torch.device, NoneType] = None, dtypes: Union[List[torch.dtype], NoneType] = None, verbose: int = 1, **kwargs: Any) -> torchsummary.model_statistics.ModelStatistics>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bbnVuzegpXs"
      },
      "source": [
        "## Clone team's code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rSPl_TQsiRgl"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/csgy6953_DeepLearning_Midterm/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw2LzDqIgoQt",
        "outputId": "c00e21df-62d0-48a1-e006-5326f38d0369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'csgy6953_DeepLearning_Midterm'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Counting objects: 100% (179/179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "remote: Total 686 (delta 90), reused 76 (delta 50), pack-reused 507\u001b[K\n",
            "Receiving objects: 100% (686/686), 319.97 KiB | 713.00 KiB/s, done.\n",
            "Resolving deltas: 100% (441/441), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b config_kernel_size_on_updated_main \"https://github.com/ab7289-tandon-nyu/csgy6953_DeepLearning_Midterm.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_buBR1H1YpEU"
      },
      "source": [
        "if running on Google Colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "17_B3oo1glfe"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/csgy6953_DeepLearning_Midterm/src/ ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmnH6onXYpEV"
      },
      "source": [
        "if running on Amazon SageMaker Studio Lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m2NARt3GYpEV"
      },
      "outputs": [],
      "source": [
        "# !cp -r csgy6953_DeepLearning_Midterm/src/ ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat src/model.py"
      ],
      "metadata": {
        "id": "8aaPKvBWaUtW",
        "outputId": "a8d7e9bc-76be-4272-ec0a-58a654463c09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from enum import Enum\n",
            "from typing import List, Optional, Tuple\n",
            "\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "\n",
            "class ResidualBlockType(Enum):\n",
            "    \"\"\"\n",
            "    Enum class to represent the residual block type for ResNet\n",
            "    \"\"\"\n",
            "\n",
            "    BASIC = 0\n",
            "    BOTTLENECK = 1\n",
            "\n",
            "\n",
            "class LayerType(Enum):\n",
            "    \"\"\"\n",
            "    Enum class to represent layer within for ResidualBlock and for BottleneckResidualBlock\n",
            "    \"\"\"\n",
            "\n",
            "    # Disambiguation: here \"layer\" refers to the individual layer within a block,\n",
            "    # not a \"residual layer\" containing one or more blocks\n",
            "    CONV = 0\n",
            "\n",
            "\n",
            "class LayerLoc(Enum):\n",
            "    \"\"\"\n",
            "    Enum class to represent a layer's location within a block\n",
            "    \"\"\"\n",
            "\n",
            "    MAIN_BLOCK_CONV1 = 0\n",
            "    MAIN_BLOCK_CONV2 = 1\n",
            "    MAIN_BLOCK_CONV3 = 2\n",
            "\n",
            "    SHORTCUT_IDENTITY = 6   # identity\n",
            "    SHORTCUT_CONV_STEM = 7\n",
            "\n",
            "\n",
            "def generate_layer(\n",
            "    block_type: ResidualBlockType,\n",
            "    layer_type: LayerType, \n",
            "    layer_loc: LayerLoc, # position of this layer within the block starting from index 1 \n",
            "    num_channels: int,\n",
            "    main_block_kernel_size: int,\n",
            "    strides: int = 1,\n",
            "    factor: int = 4,\n",
            "    use_bias: bool = False,\n",
            "):\n",
            "    \"\"\"\n",
            "    Returns a layer with the most appropriate parameters such as padding \n",
            "    \"\"\"\n",
            "    if block_type == ResidualBlockType.BASIC:\n",
            "        if layer_type == LayerType.CONV:\n",
            "\n",
            "            if main_block_kernel_size == 3:\n",
            "                layer_locator = {\n",
            "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
            "                        num_channels,\n",
            "                        kernel_size=3, padding=1, # ResidualBlock.conv1\n",
            "                        stride=strides, bias=use_bias\n",
            "                        ), \n",
            "                     LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
            "                        num_channels,\n",
            "                        kernel_size=3, padding=1, # ResidualBlock.conv2\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                     LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "                     LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
            "                        bias=use_bias\n",
            "                        )\n",
            "                    }\n",
            "            \n",
            "            # # partial solution 1:\n",
            "            # # fatal issue: when input is [256,1,1], MUST pad in order to apply 2x2 kernel\n",
            "            # if main_block_kernel_size == 2:\n",
            "            #     layer_locator = {\n",
            "            #         LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
            "            #             num_channels,\n",
            "            #             kernel_size=2, padding=1, # ResidualBlock.conv1\n",
            "            #             stride=strides, bias=use_bias\n",
            "            #             ), \n",
            "            #          LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
            "            #             num_channels,\n",
            "            #             kernel_size=2, padding=0, # ResidualBlock.conv2\n",
            "            #             bias=use_bias\n",
            "            #             ),\n",
            "            #          LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "            #          LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "            #             num_channels, \n",
            "            #             kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
            "            #             bias=use_bias\n",
            "            #             )\n",
            "            #         }\n",
            "            \n",
            "            # partial solution 3:\n",
            "            if main_block_kernel_size == 2:\n",
            "                layer_locator = {\n",
            "                    LayerLoc.MAIN_BLOCK_CONV1: nn.Sequential(\n",
            "                        nn.LazyConv2d(\n",
            "                            num_channels,\n",
            "                            kernel_size=2, padding=1, # ResidualBlock.conv1\n",
            "                            stride=strides, bias=use_bias\n",
            "                            ), # result: image size += 1\n",
            "                        nn.AvgPool2d(\n",
            "                            kernel_size=2, stride=1\n",
            "                            ) # result: image size -= 1\n",
            "                        ),\n",
            "                     LayerLoc.MAIN_BLOCK_CONV2: nn.Sequential(\n",
            "                        nn.LazyConv2d(\n",
            "                            num_channels,\n",
            "                            kernel_size=2, padding=1, # ResidualBlock.conv2\n",
            "                            bias=use_bias\n",
            "                            ), # result: image size += 1\n",
            "                        nn.AvgPool2d(\n",
            "                            kernel_size=2, stride=1\n",
            "                            ) # result: image size -= 1\n",
            "                        ),\n",
            "                     LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "                     LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
            "                        bias=use_bias\n",
            "                        )\n",
            "                    }\n",
            "            \n",
            "    if block_type == ResidualBlockType.BOTTLENECK:\n",
            "        if layer_type == LayerType.CONV:\n",
            "\n",
            "            if main_block_kernel_size == 3:\n",
            "                layer_locator = {\n",
            "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
            "                        num_channels // factor,\n",
            "                        kernel_size=1, padding=0, # BottleneckResidualBlock.conv1\n",
            "                        bias=use_bias\n",
            "                        ), \n",
            "                    LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
            "                        num_channels // factor,\n",
            "                        kernel_size=3, padding=1, stride=strides, # BottleneckResidualBlock.conv2\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                    LayerLoc.MAIN_BLOCK_CONV3: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, padding=0, # BottleneckResidualBlock.conv3\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                    LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "                    LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, stride=strides, # BottleneckResidualBlock.conv_stem\n",
            "                        bias=use_bias\n",
            "                        )\n",
            "                    }\n",
            "    return layer_locator[layer_loc]\n",
            "    \n",
            "\n",
            "class ResidualBlock(nn.Module):\n",
            "    \"\"\"\n",
            "    Class representing a convolutional residual block\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        num_channels: int,\n",
            "        use_stem: bool = False,\n",
            "        strides: int = 1,\n",
            "        dropout: Optional[float] = None,\n",
            "        use_bias: bool = False,\n",
            "        main_block_kernel_size: int = 3\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Creates a new instance of a Residual Block\n",
            "        @param: num_channels (int) - the number of output channels for all convolutions in\n",
            "            the block\n",
            "        @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the\n",
            "            residual\n",
            "        @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
            "        @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.num_channels = num_channels\n",
            "        self.use_stem = use_stem\n",
            "        self.strides = strides\n",
            "\n",
            "        self.dropout = nn.Dropout(dropout) if dropout is not None else None\n",
            "        self.conv1 = generate_layer(\n",
            "            block_type = ResidualBlockType.BASIC,\n",
            "            layer_type = LayerType.CONV,\n",
            "            layer_loc = LayerLoc.MAIN_BLOCK_CONV1,\n",
            "            num_channels = num_channels,\n",
            "            main_block_kernel_size = main_block_kernel_size,\n",
            "            strides = strides,\n",
            "            use_bias = use_bias,\n",
            "        )\n",
            "        self.conv2 = generate_layer(\n",
            "            block_type = ResidualBlockType.BASIC,\n",
            "            layer_type = LayerType.CONV,\n",
            "            layer_loc = LayerLoc.MAIN_BLOCK_CONV2,\n",
            "            num_channels = num_channels,\n",
            "            main_block_kernel_size = main_block_kernel_size,\n",
            "            use_bias = use_bias,\n",
            "        )\n",
            "        self.relu = nn.ReLU(inplace=True)\n",
            "        self.out = nn.ReLU(inplace=True)\n",
            "        self.bn1 = nn.LazyBatchNorm2d()\n",
            "        self.bn2 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        self.identity = None\n",
            "        self.conv_stem = None\n",
            "        if use_stem:\n",
            "            self.conv_stem = generate_layer(\n",
            "                block_type = ResidualBlockType.BASIC,\n",
            "                layer_type = LayerType.CONV,\n",
            "                layer_loc = LayerLoc.SHORTCUT_CONV_STEM,\n",
            "                num_channels = num_channels, \n",
            "                main_block_kernel_size = main_block_kernel_size,\n",
            "                strides = strides,\n",
            "                use_bias = use_bias\n",
            "            )\n",
            "        else:\n",
            "            self.identity = generate_layer(\n",
            "                block_type = ResidualBlockType.BASIC,\n",
            "                layer_type = LayerType.CONV,\n",
            "                layer_loc = LayerLoc.SHORTCUT_IDENTITY,\n",
            "                num_channels = num_channels,\n",
            "                main_block_kernel_size = main_block_kernel_size\n",
            "            )\n",
            "\n",
            "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
            "        shortcut = inputs\n",
            "\n",
            "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
            "        if self.dropout is not None:\n",
            "            x = self.dropout(x)\n",
            "        x = self.bn2(self.conv2(x))\n",
            "        \n",
            "        if self.use_stem:\n",
            "            # downsample skip connection\n",
            "            shortcut = self.conv_stem(shortcut)\n",
            "        else:\n",
            "            shortcut = self.identity(shortcut)\n",
            "\n",
            "        # add in skip connection\n",
            "        x += shortcut\n",
            "        return self.out(x)\n",
            "\n",
            "\n",
            "class BottleneckResidualBlock(nn.Module):\n",
            "    \"\"\"\n",
            "    Class representing a convolutional residual block with a bottleneck\n",
            "    This class was built with reference to:\n",
            "    https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        num_channels: int,\n",
            "        use_stem: bool = False,\n",
            "        strides: int = 1,\n",
            "        factor: int = 4,\n",
            "        dropout: Optional[float] = None,\n",
            "        use_bias: bool = False,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Creates a new instance of a Residual BottleNeck Block\n",
            "        @param: num_channels (int) - the number of output channels for all convolutions in the block\n",
            "        @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the residual\n",
            "        @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
            "        @param: factor (int) - the factor by which the input channels will be reduced for the bottleneck\n",
            "        @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.num_channels = num_channels\n",
            "        self.use_stem = use_stem\n",
            "        self.strides = strides\n",
            "        self.factor = factor\n",
            "        self.dropout1 = nn.Dropout(dropout) if dropout is not None else None\n",
            "        self.dropout2 = nn.Dropout(dropout) if dropout is not None else None\n",
            "\n",
            "        # First convolutional layer with normalization\n",
            "        self.conv1 = nn.LazyConv2d(\n",
            "            num_channels // factor, kernel_size=1, padding=0, bias=use_bias\n",
            "        )\n",
            "        self.bn1 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        # Second convolutional layer with normalization\n",
            "        self.conv2 = nn.LazyConv2d(\n",
            "            num_channels // factor,\n",
            "            kernel_size=3,\n",
            "            padding=1,\n",
            "            stride=strides,\n",
            "            bias=use_bias,\n",
            "        )\n",
            "        self.bn2 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        # Third convolutional layer with normalization\n",
            "        self.conv3 = nn.LazyConv2d(\n",
            "            num_channels, kernel_size=1, padding=0, bias=use_bias\n",
            "        )\n",
            "        self.bn3 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        self.relu = nn.ReLU(inplace=True)\n",
            "\n",
            "        self.conv_stem = None\n",
            "        if use_stem:\n",
            "            # Bottleneck residual block\n",
            "            self.conv_stem = nn.LazyConv2d(\n",
            "                num_channels, kernel_size=1, stride=strides, bias=use_bias\n",
            "            )\n",
            "\n",
            "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
            "        shortcut = inputs\n",
            "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
            "        if self.dropout1 is not None:\n",
            "            x = self.dropout1(x)\n",
            "        x = self.relu(self.bn2(self.conv2(x)))\n",
            "        if self.dropout2 is not None:\n",
            "            x = self.dropout2(x)\n",
            "        x = self.bn3(self.conv3(x))\n",
            "        if self.use_stem:\n",
            "            # downsample skip connection\n",
            "            shortcut = self.conv_stem(shortcut)\n",
            "\n",
            "        # add in skip connection\n",
            "        x += shortcut\n",
            "        return self.relu(x)\n",
            "\n",
            "\n",
            "class StemConfig:\n",
            "    \"\"\"\n",
            "    convenience class to encapsulate configuration options\n",
            "    for the ResNet stem\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, num_channels, kernel_size, stride, padding):\n",
            "        self.num_channels = num_channels\n",
            "        self.kernel_size = kernel_size\n",
            "        self.stride = stride\n",
            "        self.padding = padding\n",
            "\n",
            "\n",
            "def generate_block(\n",
            "    block_type: ResidualBlockType,\n",
            "    num_channels: int,\n",
            "    use_stem: bool = False,\n",
            "    strides: int = 1,\n",
            "    factor: int = 4,\n",
            "    dropout: Optional[float] = None,\n",
            "    use_bias: bool = False,\n",
            "    main_block_kernel_size: int = 3\n",
            "):\n",
            "    \"\"\"\n",
            "    Returns either a Residual Block or a ResidualBottleneck\n",
            "    \"\"\"\n",
            "    if block_type == ResidualBlockType.BASIC:\n",
            "        return ResidualBlock(\n",
            "            num_channels,\n",
            "            use_stem=use_stem,\n",
            "            strides=strides,\n",
            "            dropout=dropout,\n",
            "            use_bias=use_bias,\n",
            "            main_block_kernel_size=main_block_kernel_size\n",
            "        )\n",
            "    else:\n",
            "        return BottleneckResidualBlock(\n",
            "            num_channels,\n",
            "            use_stem=use_stem,\n",
            "            strides=strides,\n",
            "            factor=factor,\n",
            "            dropout=dropout,\n",
            "            use_bias=use_bias,\n",
            "            main_block_kernel_size=main_block_kernel_size\n",
            "        )\n",
            "\n",
            "\n",
            "class ResNet(nn.Module):\n",
            "    \"\"\"\n",
            "    Class representing a full ResNet model\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        architecture: List[Tuple[ResidualBlockType, int, int, float, int]],\n",
            "        stem_config: Optional[StemConfig],\n",
            "        output_size: int = 10,\n",
            "        use_bias: bool = False,\n",
            "        *args,\n",
            "        **kwargs,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        returns an instance of a ResNet\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.use_bias = use_bias\n",
            "        if stem_config is not None:\n",
            "            self.stem = self.create_stem(\n",
            "                stem_config.num_channels,\n",
            "                stem_config.kernel_size,\n",
            "                stem_config.stride,\n",
            "                stem_config.padding,\n",
            "                use_bias=use_bias,\n",
            "            )\n",
            "        else:\n",
            "            self.stem = self.create_stem(use_bias=use_bias)\n",
            "        self.classifier = self.create_classifier(output_size, use_bias=use_bias)\n",
            "\n",
            "        self.body = nn.Sequential()\n",
            "        for idx, block_def in enumerate(architecture):\n",
            "            self.body.add_module(\n",
            "                f\"block_{idx+2}\",\n",
            "                self.create_block(\n",
            "                    *block_def, first_block=(idx == 0), use_bias=use_bias\n",
            "                ),\n",
            "            )\n",
            "\n",
            "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
            "        \"\"\"\n",
            "        Performs forward pass of the inputs through the network\n",
            "        \"\"\"\n",
            "        x = self.stem(inputs)\n",
            "        x = self.body(x)\n",
            "        return self.classifier(x)\n",
            "\n",
            "    def create_stem(\n",
            "        self,\n",
            "        num_channels: int = 64,\n",
            "        kernel_size: int = 7,\n",
            "        stride: int = 2,\n",
            "        padding: int = 3,\n",
            "        use_bias: bool = False,\n",
            "    ) -> nn.Sequential:\n",
            "        \"\"\"\n",
            "        Creates a sequential stem as the first component of the model\n",
            "        \"\"\"\n",
            "        return nn.Sequential(\n",
            "            nn.LazyConv2d(\n",
            "                num_channels,\n",
            "                kernel_size=kernel_size,\n",
            "                padding=padding,\n",
            "                stride=stride,\n",
            "                bias=use_bias,\n",
            "            ),\n",
            "            nn.LazyBatchNorm2d(),\n",
            "            nn.ReLU(inplace=True),\n",
            "        )\n",
            "\n",
            "    def create_classifier(\n",
            "        self, num_classes: int, use_bias: bool = False\n",
            "    ) -> nn.Sequential:\n",
            "        \"\"\"\n",
            "        Creates a sequential classifier head at the very\n",
            "        \"\"\"\n",
            "        return nn.Sequential(\n",
            "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.LazyLinear(num_classes)\n",
            "        )\n",
            "\n",
            "    def create_block(\n",
            "        self,\n",
            "        block_type: ResidualBlockType,\n",
            "        num_residuals: int,\n",
            "        num_channels: int,\n",
            "        dropout: Optional[float] = None,\n",
            "        main_block_kernel_size: int = 3,\n",
            "        first_block: bool = False,\n",
            "        use_bias: bool = False,\n",
            "    ) -> nn.Sequential:\n",
            "        \"\"\"\n",
            "        Given our inputs, generates either a ResidualBlock or ResidualBottleNeck and addes it to our\n",
            "        sequence of layers\n",
            "        \"\"\"\n",
            "        layer = []\n",
            "        for i in range(num_residuals):\n",
            "            if i == 0 and not first_block:\n",
            "                layer.append(\n",
            "                    generate_block(\n",
            "                        block_type,\n",
            "                        num_channels,\n",
            "                        use_stem=True,\n",
            "                        strides=2,\n",
            "                        dropout=dropout,\n",
            "                        use_bias=use_bias,\n",
            "                        main_block_kernel_size=main_block_kernel_size\n",
            "                    )\n",
            "                )\n",
            "            else:\n",
            "                layer.append(\n",
            "                    generate_block(\n",
            "                        block_type, num_channels, dropout=dropout, use_bias=use_bias, \n",
            "                        main_block_kernel_size=main_block_kernel_size\n",
            "                    )\n",
            "                )\n",
            "        return nn.Sequential(*layer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geVocFZngwER"
      },
      "source": [
        "# Import, Seed, Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Yk6T4FzhgvGv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-QebqUCUgyc-"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lsMOOo-gzTu",
        "outputId": "8c091a32-0de3-4b65-9b6c-653b26561b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br5Rha2PhBoT"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD28XJ_qg1Ee",
        "outputId": "1d32a205-786d-4c42-adde-376cb669753d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from src.data import get_transformed_data, make_data_loaders\n",
        "from src.transforms import make_auto_transforms # used to use: make_transforms\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "VALID_RATIO = 0.1\n",
        "\n",
        "train_data, valid_data, test_data = \\\n",
        "get_transformed_data(make_transforms=make_auto_transforms, valid_ratio=VALID_RATIO)\n",
        "\n",
        "train_iter, valid_iter, test_iter = \\\n",
        "make_data_loaders(train_data, valid_data, test_data, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P0KpocliyKi"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: import"
      ],
      "metadata": {
        "id": "09YbNCaCYxMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.model import StemConfig, ResidualBlockType, ResNet"
      ],
      "metadata": {
        "id": "jCSim-9qY1V4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: try code here\n",
        "`config_kernel_size_on_updated_main` branch > src > model.py"
      ],
      "metadata": {
        "id": "MrDIWjVaY2PV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F10Lm95gY5sG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftpl9IQRYpEX"
      },
      "source": [
        "## Architecture <<<"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ORrGNYQGix9m"
      },
      "outputs": [],
      "source": [
        "DROPOUT = 0.1\n",
        "MAIN_BLOCK_KERNEL_SIZE = 2\n",
        "\n",
        "# 1 tuple == 1 layer\n",
        "# block with or without bottleneck, how many blocks in each layer, out_channels, dropout prob, kernel_size\n",
        "architecture = [\n",
        "    (ResidualBlockType.BASIC, 2,  64, DROPOUT, MAIN_BLOCK_KERNEL_SIZE),\n",
        "    (ResidualBlockType.BASIC, 2, 128, DROPOUT, MAIN_BLOCK_KERNEL_SIZE),\n",
        "    (ResidualBlockType.BASIC, 2, 256, DROPOUT, MAIN_BLOCK_KERNEL_SIZE),\n",
        "    (ResidualBlockType.BASIC, 2, 512, DROPOUT, MAIN_BLOCK_KERNEL_SIZE),\n",
        "]\n",
        "\n",
        "# 2022.11/18(5)_p03.59 (Oscar)\n",
        "# slightly reduce the last layer's out_channels to keep overall params under 5M:\n",
        "# 512 -> 5,069,130\n",
        "# 508 -> 5,014,978\n",
        "# 507 -> 5,001,500\n",
        "# 506 -> 4,988,046 <<<"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Bz6S1KmzOjda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997a5b05-9044-4a89-c31a-436c15e92913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "config = StemConfig(num_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "model  = ResNet(architecture, stem_config=config, output_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LwVNRxIUnAJ"
      },
      "source": [
        "## Initialize model shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Dw4mJFmkKu",
        "outputId": "f58e0e50-60d2-4971-fec8-2553d3368dd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "-------------------\n",
            "ResNet(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            "  (body): Sequential(\n",
            "    (block_2): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "    (block_3): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "    (block_4): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "    (block_5): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "(5069130, 5069130)\n",
            "torch.Size([128, 10])\n"
          ]
        }
      ],
      "source": [
        "# intialize a new model\n",
        "\n",
        "inputs = torch.empty((BATCH_SIZE, 3, 32, 32)) #  passed\n",
        "inputs.normal_()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "outputs = model(inputs.to(device)) # internally converts all nn.LazyConv2d layers to nn.Conv2d\n",
        "\n",
        "print('-------------------')\n",
        "print('-------------------')\n",
        "\n",
        "print(model)\n",
        "\n",
        "from src.utils import count_parameters\n",
        "\n",
        "print(count_parameters(model))\n",
        "print(outputs.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWvfEz42UfD0"
      },
      "source": [
        "## Count parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJeHP8QWT_mQ",
        "outputId": "21965766-8956-40b4-f3f1-a612c337de10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5069130\n"
          ]
        }
      ],
      "source": [
        "num_parameters, num_parameters_requiring_grad = count_parameters(model)\n",
        "print(num_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuhd85NQdSVI",
        "outputId": "9773c177-2e04-4ad2-eb7a-1b2928949e4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Sequential: 1-1                        [-1, 64, 32, 32]          --\n",
              "|    Conv2d: 2-1                       [-1, 64, 32, 32]          1,728\n",
              "|    BatchNorm2d: 2-2                  [-1, 64, 32, 32]          128\n",
              "|    ReLU: 2-3                         [-1, 64, 32, 32]          --\n",
              "Sequential: 1-2                        [-1, 512, 4, 4]           --\n",
              "|    Sequential: 2-4                   [-1, 64, 32, 32]          --\n",
              "|    |    ResidualBlock: 3-1           [-1, 64, 32, 32]          33,024\n",
              "|    |    ResidualBlock: 3-2           [-1, 64, 32, 32]          33,024\n",
              "|    Sequential: 2-5                   [-1, 128, 16, 16]         --\n",
              "|    |    ResidualBlock: 3-3           [-1, 128, 16, 16]         107,008\n",
              "|    |    ResidualBlock: 3-4           [-1, 128, 16, 16]         131,584\n",
              "|    Sequential: 2-6                   [-1, 256, 8, 8]           --\n",
              "|    |    ResidualBlock: 3-5           [-1, 256, 8, 8]           427,008\n",
              "|    |    ResidualBlock: 3-6           [-1, 256, 8, 8]           525,312\n",
              "|    Sequential: 2-7                   [-1, 512, 4, 4]           --\n",
              "|    |    ResidualBlock: 3-7           [-1, 512, 4, 4]           1,705,984\n",
              "|    |    ResidualBlock: 3-8           [-1, 512, 4, 4]           2,099,200\n",
              "Sequential: 1-3                        [-1, 10]                  --\n",
              "|    AdaptiveAvgPool2d: 2-8            [-1, 512, 1, 1]           --\n",
              "|    Flatten: 2-9                      [-1, 512]                 --\n",
              "|    Linear: 2-10                      [-1, 10]                  5,130\n",
              "==========================================================================================\n",
              "Total params: 5,069,130\n",
              "Trainable params: 5,069,130\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 28.13\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 5.19\n",
              "Params size (MB): 19.34\n",
              "Estimated Total Size (MB): 24.54\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "QUIET_VERBOSE = 0\n",
        "\n",
        "one_sample_input_shape = (3, 32, 32)\n",
        "\n",
        "summary(model, one_sample_input_shape, verbose = QUIET_VERBOSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQKBIE8_UgoP"
      },
      "source": [
        "## Initialize parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uXBP9Cm0Up-h"
      },
      "outputs": [],
      "source": [
        "from src.utils import initialize_parameters\n",
        "\n",
        "# initialize model weights\n",
        "model.apply(initialize_parameters);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xAeC3JkTZPn"
      },
      "source": [
        "# Train, validate, log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbTgjcLMYpEa"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SNYcxMz6YpEa"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RticcmCFYpEa"
      },
      "source": [
        "## Optimizer, scheduler (learning rate) <<<"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oOacAoIeYpEb"
      },
      "outputs": [],
      "source": [
        "init_learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=init_learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "# base_lr, max_lr, lr_schedule = 0.2, 1.1, 'CyclicLR_base_0.2_max_1.1'\n",
        "base_lr, max_lr, lr_schedule = 0.02, 1.1, 'CyclicLR_base_0.02_max_1.1'\n",
        "\n",
        "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, cycle_momentum=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSDlhXutYpEb"
      },
      "source": [
        "## Iterate!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "G8RjsKs0ZjgD"
      },
      "outputs": [],
      "source": [
        "from src.engine import train_one_epoch, evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gvkyK2UgYpEb"
      },
      "outputs": [],
      "source": [
        "EPOCHS  = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwTSlzxwTwLf",
        "outputId": "055dd04d-6db4-429c-9fa3-187b8753c186"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'architecture': [(<ResidualBlockType.BASIC: 0>, 2, 64, 0.1, 2),\n",
              "  (<ResidualBlockType.BASIC: 0>, 2, 128, 0.1, 2),\n",
              "  (<ResidualBlockType.BASIC: 0>, 2, 256, 0.1, 2),\n",
              "  (<ResidualBlockType.BASIC: 0>, 2, 512, 0.1, 2)],\n",
              " 'num_params': 5069130,\n",
              " 'grad_params': 5069130,\n",
              " 'main_block_kernel_size': 2,\n",
              " 'dropout': 0.1,\n",
              " 'init_learning_rate': 0.01,\n",
              " 'base_lr': 0.02,\n",
              " 'lr_schedule': 'CyclicLR_base_0.02_max_1.1',\n",
              " 'max_lr': 1.1,\n",
              " 'batch_size': 128,\n",
              " 'epochs': 100}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "## setup wandb logging\n",
        "\n",
        "assert 'wandb' in locals() # 'wandb' has been imported already\n",
        "\n",
        "ENTITY_NAME  = 'dlf22_mini_project' # this is our team name\n",
        "\n",
        "# PROJECT_NAME = 'Junk_Test_Project'\n",
        "PROJECT_NAME = 'ResNet_5M'\n",
        "\n",
        "# RUN_NAME     = 'resnet_osca_kernel_size_2_2022.11.18.p04.13'\n",
        "# RUN_NAME = 'osca_kernel_size_2_avgpool_replicate_resnet18_adamw_0.2baselr'\n",
        "RUN_NAME = 'osca_kernel_size_2_avgpool_replicate_resnet18_adamw_0.02baselr'\n",
        "\n",
        "WANDB_CONFIG = \\\n",
        "{\"architecture\"          : architecture,                  # the model\n",
        " \"num_params\"            : num_parameters,                # the model\n",
        " \"grad_params\"           : num_parameters_requiring_grad, # the model\n",
        " \"main_block_kernel_size\": MAIN_BLOCK_KERNEL_SIZE,        # the model\n",
        " \"dropout\"               : DROPOUT,                       # the model\n",
        " \"init_learning_rate\" : init_learning_rate, # how to gradient descent\n",
        " \"base_lr\"            : base_lr,            # how to gradient descent\n",
        " \"lr_schedule\"        : lr_schedule,        # how to gradient descent\n",
        " \"max_lr\"             : max_lr,             # how to gradient descent\n",
        " \"batch_size\"         : BATCH_SIZE,         # how to gradient descent\n",
        " \"epochs\"             : EPOCHS,}   # train for how long\n",
        "\n",
        "WANDB_CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hAHgrLh_YpEc",
        "outputId": "bf735199-9ce6-4008-b899-41662df1c2d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mos2000os\u001b[0m (\u001b[33mdlf22_mini_project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221120_153624-cf141y6v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/dlf22_mini_project/ResNet_5M/runs/cf141y6v\" target=\"_blank\">osca_kernel_size_2_avgpool_replicate_resnet18_adamw_0.02baselr</a></strong> to <a href=\"https://wandb.ai/dlf22_mini_project/ResNet_5M\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dlf22_mini_project/ResNet_5M/runs/cf141y6v?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7eff8a364890>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "wandb\\\n",
        ".init(name=RUN_NAME, project=PROJECT_NAME, entity=ENTITY_NAME,config=WANDB_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nMyGRL1GUGXH",
        "outputId": "22db565b-fa53-4603-b3f2-bfc985da9413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "osca_kernel_size_2_avgpool_replicate_resnet18_adamw_0.02baselr.pt\n"
          ]
        }
      ],
      "source": [
        "# save to disk the \"best\" model === the model with the lowest validation loss\n",
        "best_model_path = RUN_NAME + '.pt'\n",
        "print(best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AbWjFOiMULJ3",
        "outputId": "41483024-443e-47f0-b964-4cdf7c9e988f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "\tTrain elapsed: 1:6, loss: 2.7274, acc: 16.64%\n",
            "\tValidation elapsed: 0:1, loss: 1.9141, acc: 25.57%\n",
            "Epoch 2\n",
            "\tTrain elapsed: 1:6, loss: 1.8774, acc: 30.06%\n",
            "\tValidation elapsed: 0:1, loss: 1.6366, acc: 38.67%\n",
            "Epoch 3\n",
            "\tTrain elapsed: 1:6, loss: 1.7312, acc: 36.23%\n",
            "\tValidation elapsed: 0:1, loss: 1.5558, acc: 41.91%\n",
            "Epoch 4\n",
            "\tTrain elapsed: 1:6, loss: 1.5957, acc: 41.97%\n",
            "\tValidation elapsed: 0:1, loss: 1.3256, acc: 50.64%\n",
            "Epoch 5\n",
            "\tTrain elapsed: 1:6, loss: 1.5019, acc: 45.49%\n",
            "\tValidation elapsed: 0:1, loss: 1.2963, acc: 52.85%\n",
            "Epoch 6\n",
            "\tTrain elapsed: 1:6, loss: 1.4177, acc: 48.90%\n",
            "\tValidation elapsed: 0:1, loss: 1.3282, acc: 50.90%\n",
            "Epoch 7\n",
            "\tTrain elapsed: 1:6, loss: 1.3467, acc: 51.73%\n",
            "\tValidation elapsed: 0:1, loss: 1.1296, acc: 59.88%\n",
            "Epoch 8\n",
            "\tTrain elapsed: 1:6, loss: 1.2957, acc: 53.67%\n",
            "\tValidation elapsed: 0:1, loss: 1.1337, acc: 59.55%\n",
            "Epoch 9\n",
            "\tTrain elapsed: 1:6, loss: 1.2527, acc: 55.28%\n",
            "\tValidation elapsed: 0:1, loss: 1.3688, acc: 50.94%\n",
            "Epoch 10\n",
            "\tTrain elapsed: 1:6, loss: 1.2233, acc: 56.51%\n",
            "\tValidation elapsed: 0:1, loss: 1.2483, acc: 56.37%\n",
            "Epoch 11\n",
            "\tTrain elapsed: 1:6, loss: 1.1904, acc: 57.83%\n",
            "\tValidation elapsed: 0:1, loss: 1.1900, acc: 57.97%\n",
            "Epoch 12\n",
            "\tTrain elapsed: 1:6, loss: 1.1712, acc: 58.46%\n",
            "\tValidation elapsed: 0:1, loss: 1.0831, acc: 62.42%\n",
            "Epoch 13\n",
            "\tTrain elapsed: 1:6, loss: 1.1468, acc: 59.30%\n",
            "\tValidation elapsed: 0:1, loss: 1.1498, acc: 59.04%\n",
            "Epoch 14\n",
            "\tTrain elapsed: 1:5, loss: 1.1241, acc: 60.30%\n",
            "\tValidation elapsed: 0:1, loss: 1.0698, acc: 62.38%\n",
            "Epoch 15\n",
            "\tTrain elapsed: 1:5, loss: 1.1081, acc: 60.76%\n",
            "\tValidation elapsed: 0:1, loss: 1.0450, acc: 64.71%\n",
            "Epoch 16\n",
            "\tTrain elapsed: 1:6, loss: 1.0863, acc: 61.76%\n",
            "\tValidation elapsed: 0:1, loss: 0.9798, acc: 65.84%\n",
            "Epoch 17\n",
            "\tTrain elapsed: 1:5, loss: 1.0538, acc: 62.77%\n",
            "\tValidation elapsed: 0:1, loss: 0.9343, acc: 66.80%\n",
            "Epoch 18\n",
            "\tTrain elapsed: 1:5, loss: 1.0447, acc: 63.35%\n",
            "\tValidation elapsed: 0:1, loss: 1.0178, acc: 64.86%\n",
            "Epoch 19\n",
            "\tTrain elapsed: 1:6, loss: 1.0291, acc: 63.93%\n",
            "\tValidation elapsed: 0:1, loss: 1.4334, acc: 54.88%\n",
            "Epoch 20\n",
            "\tTrain elapsed: 1:6, loss: 1.0393, acc: 63.56%\n",
            "\tValidation elapsed: 0:1, loss: 0.8524, acc: 69.96%\n",
            "Epoch 21\n",
            "\tTrain elapsed: 1:6, loss: 1.0251, acc: 63.91%\n",
            "\tValidation elapsed: 0:1, loss: 0.9042, acc: 68.03%\n",
            "Epoch 22\n",
            "\tTrain elapsed: 1:5, loss: 1.0162, acc: 64.36%\n",
            "\tValidation elapsed: 0:1, loss: 0.8616, acc: 69.75%\n",
            "Epoch 23\n",
            "\tTrain elapsed: 1:6, loss: 1.0097, acc: 64.65%\n",
            "\tValidation elapsed: 0:1, loss: 0.9226, acc: 68.38%\n",
            "Epoch 24\n",
            "\tTrain elapsed: 1:5, loss: 1.0122, acc: 64.61%\n",
            "\tValidation elapsed: 0:1, loss: 1.0110, acc: 65.72%\n",
            "Epoch 25\n",
            "\tTrain elapsed: 1:5, loss: 0.9969, acc: 64.96%\n",
            "\tValidation elapsed: 0:1, loss: 0.9285, acc: 67.54%\n",
            "Epoch 26\n",
            "\tTrain elapsed: 1:5, loss: 0.9930, acc: 65.26%\n",
            "\tValidation elapsed: 0:1, loss: 1.1775, acc: 60.55%\n",
            "Epoch 27\n",
            "\tTrain elapsed: 1:6, loss: 0.9907, acc: 65.38%\n",
            "\tValidation elapsed: 0:1, loss: 0.9359, acc: 68.28%\n",
            "Epoch 28\n",
            "\tTrain elapsed: 1:5, loss: 1.0046, acc: 64.92%\n",
            "\tValidation elapsed: 0:1, loss: 0.9401, acc: 67.77%\n",
            "Epoch 29\n",
            "\tTrain elapsed: 1:6, loss: 0.9879, acc: 65.61%\n",
            "\tValidation elapsed: 0:1, loss: 1.0219, acc: 64.20%\n",
            "Epoch 30\n",
            "\tTrain elapsed: 1:6, loss: 0.9839, acc: 65.54%\n",
            "\tValidation elapsed: 0:1, loss: 0.8536, acc: 70.80%\n",
            "Epoch 31\n",
            "\tTrain elapsed: 1:5, loss: 0.9899, acc: 65.35%\n",
            "\tValidation elapsed: 0:1, loss: 0.9011, acc: 69.20%\n",
            "Epoch 32\n",
            "\tTrain elapsed: 1:5, loss: 0.9854, acc: 65.62%\n",
            "\tValidation elapsed: 0:1, loss: 0.8877, acc: 68.81%\n",
            "Epoch 33\n",
            "\tTrain elapsed: 1:6, loss: 0.9999, acc: 65.11%\n",
            "\tValidation elapsed: 0:1, loss: 0.9383, acc: 67.66%\n",
            "Epoch 34\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1a58182ced22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# log & echo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_secs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/src/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, iterator, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         image = image.to(\n\u001b[1;32m     24\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2735\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2612\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2614\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2615\u001b[0m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2616\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2556\u001b[0m     \"\"\"\n\u001b[1;32m   2557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2558\u001b[0;31m     \u001b[0m_check_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_check_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2535\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Size must be a tuple of length 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2536\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2537\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Width and height must be >= 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from src.utils import epoch_time\n",
        "\n",
        "FIRST_EPOCH = 1\n",
        "\n",
        "best_loss = float('inf')\n",
        "for epoch in range(FIRST_EPOCH, FIRST_EPOCH + EPOCHS):\n",
        "\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    \n",
        "    ## TRAIN\n",
        "    start = time.time()\n",
        "    # train\n",
        "    train_loss, train_acc  = train_one_epoch(model, train_iter, criterion, optimizer, device)\n",
        "    # log & echo\n",
        "    train_mins, train_secs = epoch_time(start, time.time())\n",
        "    wandb.log({\"train_loss\": train_loss,\n",
        "               \"train_acc\" : train_acc,\n",
        "               \"epoch\"     : epoch})\n",
        "    print(f\"\\tTrain elapsed: {train_mins}:{train_secs}, loss: {train_loss:.4f}, acc: {train_acc * 100:.2f}%\")\n",
        "\n",
        "    ## VALIDATE\n",
        "    start = time.time()\n",
        "    # validate\n",
        "    val_loss, val_acc  = evaluate(model, valid_iter, criterion, device)\n",
        "    # log & echo   \n",
        "    val_mins, val_secs = epoch_time(start, time.time())\n",
        "    wandb.log({\"val_loss\": val_loss,\n",
        "               \"val_acc\" : val_acc,\n",
        "               \"epoch\"   : epoch,})\n",
        "    print(f\"\\tValidation elapsed: {val_mins}:{val_secs}, loss: {val_loss:.4f}, acc: {val_acc * 100:.2f}%\")\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    wandb.log({\"current_lr\": current_lr,\n",
        "               \"current_learning_rate\": current_lr,\n",
        "               \"epoch\"     : epoch})\n",
        "    \n",
        "    ## Memorize \"best\" model === the model with the lowest validation loss\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGr403ktYpEc"
      },
      "source": [
        "# Test, log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "N0myntf_YpEd",
        "outputId": "3556e318-be1e-427f-ff08-4964496dd46d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "osca_kernel_size_2_avgpool_replicate_resnet18_adamw_0.02baselr.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "print(best_model_path),\n",
        "model.load_state_dict(torch.load(best_model_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uuDFGLo_YpEd",
        "outputId": "695921ec-f9d6-429c-8e9a-4f224abf311e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.8516\n",
            "Test Accuracy: 69.82%\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = evaluate(model.to(device), test_iter, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}\\nTest Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_acc\" : test_acc,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EXJRW1GLYpEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51433bf5-74ee-40e2-ccee-a4bb4c5e2718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/20(Sun)_11:14:12\n"
          ]
        }
      ],
      "source": [
        "print(datetime.now(pytz.timezone('America/New_York')).strftime('%m/%d(%a)_%H:%M:%S'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Prx9paEs0YXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}