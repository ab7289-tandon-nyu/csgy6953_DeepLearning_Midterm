{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ab7289-tandon-nyu/csgy6953_DeepLearning_Midterm/blob/oscar2/notebooks/oscar_play_osca_kernel_size_2_avgpool_replicate_resnet18_adamw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HqUIrHzYpEF"
      },
      "source": [
        "# Clone, Install Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD8cBXZ_YpEJ"
      },
      "source": [
        "## Timestamp this run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MnxcGBFYpEL",
        "outputId": "021fa617-7920-40bd-8fe5-86e2c7745a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (2022.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfovHN-sl04P",
        "outputId": "200a3e9c-1f64-42ef-a3cb-fd51fd8124d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/19(Sat)_15:13:46\n"
          ]
        }
      ],
      "source": [
        "# reference: https://www.programiz.com/python-programming/datetime/current-time\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "print(datetime.now(pytz.timezone('America/New_York')).strftime('%m/%d(%a)_%H:%M:%S'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEg6gGl4gdFv"
      },
      "source": [
        "# Wandb install, login, import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrgALvYPgMBy",
        "outputId": "f9900993-6d2c-442a-bd7a-15f4a3907ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBNArx2fgf-v",
        "outputId": "f5e2563c-7e43-4134-aa1e-bf018695a44d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login \"6f19b1e6735ebc69af24f18d5b426262416027fb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MP8Tcl1ogjVP"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFOQTotOdDmI"
      },
      "source": [
        "## Torchsummary Install, Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "293u_6mjdMDE",
        "outputId": "d42e35c1-d688-4bed-e9d5-219cb3843340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-summary==1.4.5 in /usr/local/lib/python3.7/dist-packages (1.4.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-summary==1.4.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8mq4tcWdOSP",
        "outputId": "675b1bf8-5148-437c-aeff-75fdfc93b6c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function torchsummary.torchsummary.summary(model: torch.nn.modules.module.Module, input_data: Union[torch.Tensor, torch.Size, Sequence[torch.Tensor], Sequence[Union[int, Sequence[Any], torch.Size]], NoneType] = None, *args: Any, batch_dim: Union[int, NoneType] = 0, branching: bool = True, col_names: Union[Iterable[str], NoneType] = None, col_width: int = 25, depth: int = 3, device: Union[torch.device, NoneType] = None, dtypes: Union[List[torch.dtype], NoneType] = None, verbose: int = 1, **kwargs: Any) -> torchsummary.model_statistics.ModelStatistics>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bbnVuzegpXs"
      },
      "source": [
        "## Clone team's code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rSPl_TQsiRgl"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/csgy6953_DeepLearning_Midterm/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw2LzDqIgoQt",
        "outputId": "dabd4596-de23-4222-df65-7c1e9b029ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'csgy6953_DeepLearning_Midterm'...\n",
            "remote: Enumerating objects: 670, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 670 (delta 79), reused 73 (delta 48), pack-reused 507\u001b[K\n",
            "Receiving objects: 100% (670/670), 258.13 KiB | 7.17 MiB/s, done.\n",
            "Resolving deltas: 100% (430/430), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b config_kernel_size_on_updated_main \"https://github.com/ab7289-tandon-nyu/csgy6953_DeepLearning_Midterm.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_buBR1H1YpEU"
      },
      "source": [
        "if running on Google Colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "17_B3oo1glfe"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/csgy6953_DeepLearning_Midterm/src/ ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmnH6onXYpEV"
      },
      "source": [
        "if running on Amazon SageMaker Studio Lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m2NARt3GYpEV"
      },
      "outputs": [],
      "source": [
        "# !cp -r csgy6953_DeepLearning_Midterm/src/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aaPKvBWaUtW",
        "outputId": "0cece0d5-6902-45ae-cbe2-3778d102678f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from enum import Enum\n",
            "from typing import List, Optional, Tuple\n",
            "\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "\n",
            "class ResidualBlockType(Enum):\n",
            "    \"\"\"\n",
            "    Enum class to represent the residual block type for ResNet\n",
            "    \"\"\"\n",
            "\n",
            "    BASIC = 0\n",
            "    BOTTLENECK = 1\n",
            "\n",
            "\n",
            "class LayerType(Enum):\n",
            "    \"\"\"\n",
            "    Enum class to represent layer within for ResidualBlock and for BottleneckResidualBlock\n",
            "    \"\"\"\n",
            "\n",
            "    # Disambiguation: here \"layer\" refers to the individual layer within a block,\n",
            "    # not a \"residual layer\" containing one or more blocks\n",
            "    CONV = 0\n",
            "\n",
            "\n",
            "class LayerLoc(Enum):\n",
            "    \"\"\"\n",
            "    Enum class to represent a layer's location within a block\n",
            "    \"\"\"\n",
            "\n",
            "    MAIN_BLOCK_CONV1 = 0\n",
            "    MAIN_BLOCK_CONV2 = 1\n",
            "    MAIN_BLOCK_CONV3 = 2\n",
            "\n",
            "    SHORTCUT_IDENTITY = 6   # identity\n",
            "    SHORTCUT_CONV_STEM = 7\n",
            "\n",
            "\n",
            "def generate_layer(\n",
            "    block_type: ResidualBlockType,\n",
            "    layer_type: LayerType, \n",
            "    layer_loc: LayerLoc, # position of this layer within the block starting from index 1 \n",
            "    num_channels: int,\n",
            "    main_block_kernel_size: int,\n",
            "    strides: int = 1,\n",
            "    factor: int = 4,\n",
            "    use_bias: bool = False,\n",
            "):\n",
            "    \"\"\"\n",
            "    Returns a layer with the most appropriate parameters such as padding \n",
            "    \"\"\"\n",
            "    if block_type == ResidualBlockType.BASIC:\n",
            "        if layer_type == LayerType.CONV:\n",
            "\n",
            "            if main_block_kernel_size == 3:\n",
            "                layer_locator = {\n",
            "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
            "                        num_channels,\n",
            "                        kernel_size=3, padding=1, # ResidualBlock.conv1\n",
            "                        stride=strides, bias=use_bias\n",
            "                        ), \n",
            "                     LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
            "                        num_channels,\n",
            "                        kernel_size=3, padding=1, # ResidualBlock.conv2\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                     LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "                     LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
            "                        bias=use_bias\n",
            "                        )\n",
            "                    }\n",
            "            \n",
            "            # # partial solution 1:\n",
            "            # # fatal issue: when input is [256,1,1], MUST pad in order to apply 2x2 kernel\n",
            "            # if main_block_kernel_size == 2:\n",
            "            #     layer_locator = {\n",
            "            #         LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
            "            #             num_channels,\n",
            "            #             kernel_size=2, padding=1, # ResidualBlock.conv1\n",
            "            #             stride=strides, bias=use_bias\n",
            "            #             ), \n",
            "            #          LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
            "            #             num_channels,\n",
            "            #             kernel_size=2, padding=0, # ResidualBlock.conv2\n",
            "            #             bias=use_bias\n",
            "            #             ),\n",
            "            #          LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "            #          LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "            #             num_channels, \n",
            "            #             kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
            "            #             bias=use_bias\n",
            "            #             )\n",
            "            #         }\n",
            "            \n",
            "            # partial solution 3:\n",
            "            if main_block_kernel_size == 2:\n",
            "                layer_locator = {\n",
            "                    LayerLoc.MAIN_BLOCK_CONV1: nn.Sequential(\n",
            "                        nn.LazyConv2d(\n",
            "                            num_channels,\n",
            "                            kernel_size=2, padding=1, # ResidualBlock.conv1\n",
            "                            stride=strides, bias=use_bias\n",
            "                            ), # result: image size += 1\n",
            "                        nn.AvgPool2d(\n",
            "                            kernel_size=2, stride=1\n",
            "                            ) # result: image size -= 1\n",
            "                        ),\n",
            "                     LayerLoc.MAIN_BLOCK_CONV2: nn.Sequential(\n",
            "                        nn.LazyConv2d(\n",
            "                            num_channels,\n",
            "                            kernel_size=2, padding=1, # ResidualBlock.conv2\n",
            "                            bias=use_bias\n",
            "                            ), # result: image size += 1\n",
            "                        nn.AvgPool2d(\n",
            "                            kernel_size=2, stride=1\n",
            "                            ) # result: image size -= 1\n",
            "                        ),\n",
            "                     LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "                     LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
            "                        bias=use_bias\n",
            "                        )\n",
            "                    }\n",
            "            \n",
            "    if block_type == ResidualBlockType.BOTTLENECK:\n",
            "        if layer_type == LayerType.CONV:\n",
            "\n",
            "            if main_block_kernel_size == 3:\n",
            "                layer_locator = {\n",
            "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
            "                        num_channels // factor,\n",
            "                        kernel_size=1, padding=0, # BottleneckResidualBlock.conv1\n",
            "                        bias=use_bias\n",
            "                        ), \n",
            "                    LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
            "                        num_channels // factor,\n",
            "                        kernel_size=3, padding=1, stride=strides, # BottleneckResidualBlock.conv2\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                    LayerLoc.MAIN_BLOCK_CONV3: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, padding=0, # BottleneckResidualBlock.conv3\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                    LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "                    LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, stride=strides, # BottleneckResidualBlock.conv_stem\n",
            "                        bias=use_bias\n",
            "                        )\n",
            "                    }\n",
            "    return layer_locator[layer_loc]\n",
            "    \n",
            "\n",
            "class ResidualBlock(nn.Module):\n",
            "    \"\"\"\n",
            "    Class representing a convolutional residual block\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        num_channels: int,\n",
            "        use_stem: bool = False,\n",
            "        strides: int = 1,\n",
            "        dropout: Optional[float] = None,\n",
            "        use_bias: bool = False,\n",
            "        main_block_kernel_size: int = 3\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Creates a new instance of a Residual Block\n",
            "        @param: num_channels (int) - the number of output channels for all convolutions in\n",
            "            the block\n",
            "        @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the\n",
            "            residual\n",
            "        @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
            "        @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.num_channels = num_channels\n",
            "        self.use_stem = use_stem\n",
            "        self.strides = strides\n",
            "\n",
            "        self.dropout = nn.Dropout(dropout) if dropout is not None else None\n",
            "        self.conv1 = generate_layer(\n",
            "            block_type = ResidualBlockType.BASIC,\n",
            "            layer_type = LayerType.CONV,\n",
            "            layer_loc = LayerLoc.MAIN_BLOCK_CONV1,\n",
            "            num_channels = num_channels,\n",
            "            main_block_kernel_size = main_block_kernel_size,\n",
            "            strides = strides,\n",
            "            use_bias = use_bias,\n",
            "        )\n",
            "        self.conv2 = generate_layer(\n",
            "            block_type = ResidualBlockType.BASIC,\n",
            "            layer_type = LayerType.CONV,\n",
            "            layer_loc = LayerLoc.MAIN_BLOCK_CONV2,\n",
            "            num_channels = num_channels,\n",
            "            main_block_kernel_size = main_block_kernel_size,\n",
            "            use_bias = use_bias,\n",
            "        )\n",
            "        self.relu = nn.ReLU(inplace=True)\n",
            "        self.out = nn.ReLU(inplace=True)\n",
            "        self.bn1 = nn.LazyBatchNorm2d()\n",
            "        self.bn2 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        self.identity = None\n",
            "        self.conv_stem = None\n",
            "        if use_stem:\n",
            "            self.conv_stem = generate_layer(\n",
            "                block_type = ResidualBlockType.BASIC,\n",
            "                layer_type = LayerType.CONV,\n",
            "                layer_loc = LayerLoc.SHORTCUT_CONV_STEM,\n",
            "                num_channels = num_channels, \n",
            "                main_block_kernel_size = main_block_kernel_size,\n",
            "                strides = strides,\n",
            "                use_bias = use_bias\n",
            "            )\n",
            "        else:\n",
            "            self.identity = generate_layer(\n",
            "                block_type = ResidualBlockType.BASIC,\n",
            "                layer_type = LayerType.CONV,\n",
            "                layer_loc = LayerLoc.SHORTCUT_IDENTITY,\n",
            "                num_channels = num_channels,\n",
            "                main_block_kernel_size = main_block_kernel_size\n",
            "            )\n",
            "\n",
            "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
            "        shortcut = inputs\n",
            "\n",
            "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
            "        if self.dropout is not None:\n",
            "            x = self.dropout(x)\n",
            "        x = self.bn2(self.conv2(x))\n",
            "        \n",
            "        if self.use_stem:\n",
            "            # downsample skip connection\n",
            "            shortcut = self.conv_stem(shortcut)\n",
            "        else:\n",
            "            shortcut = self.identity(shortcut)\n",
            "\n",
            "        # add in skip connection\n",
            "        x += shortcut\n",
            "        return self.out(x)\n",
            "\n",
            "\n",
            "class BottleneckResidualBlock(nn.Module):\n",
            "    \"\"\"\n",
            "    Class representing a convolutional residual block with a bottleneck\n",
            "    This class was built with reference to:\n",
            "    https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        num_channels: int,\n",
            "        use_stem: bool = False,\n",
            "        strides: int = 1,\n",
            "        factor: int = 4,\n",
            "        dropout: Optional[float] = None,\n",
            "        use_bias: bool = False,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Creates a new instance of a Residual BottleNeck Block\n",
            "        @param: num_channels (int) - the number of output channels for all convolutions in the block\n",
            "        @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the residual\n",
            "        @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
            "        @param: factor (int) - the factor by which the input channels will be reduced for the bottleneck\n",
            "        @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.num_channels = num_channels\n",
            "        self.use_stem = use_stem\n",
            "        self.strides = strides\n",
            "        self.factor = factor\n",
            "        self.dropout1 = nn.Dropout(dropout) if dropout is not None else None\n",
            "        self.dropout2 = nn.Dropout(dropout) if dropout is not None else None\n",
            "\n",
            "        # First convolutional layer with normalization\n",
            "        self.conv1 = nn.LazyConv2d(\n",
            "            num_channels // factor, kernel_size=1, padding=0, bias=use_bias\n",
            "        )\n",
            "        self.bn1 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        # Second convolutional layer with normalization\n",
            "        self.conv2 = nn.LazyConv2d(\n",
            "            num_channels // factor,\n",
            "            kernel_size=3,\n",
            "            padding=1,\n",
            "            stride=strides,\n",
            "            bias=use_bias,\n",
            "        )\n",
            "        self.bn2 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        # Third convolutional layer with normalization\n",
            "        self.conv3 = nn.LazyConv2d(\n",
            "            num_channels, kernel_size=1, padding=0, bias=use_bias\n",
            "        )\n",
            "        self.bn3 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        self.relu = nn.ReLU(inplace=True)\n",
            "\n",
            "        self.conv_stem = None\n",
            "        if use_stem:\n",
            "            # Bottleneck residual block\n",
            "            self.conv_stem = nn.LazyConv2d(\n",
            "                num_channels, kernel_size=1, stride=strides, bias=use_bias\n",
            "            )\n",
            "\n",
            "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
            "        shortcut = inputs\n",
            "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
            "        if self.dropout1 is not None:\n",
            "            x = self.dropout1(x)\n",
            "        x = self.relu(self.bn2(self.conv2(x)))\n",
            "        if self.dropout2 is not None:\n",
            "            x = self.dropout2(x)\n",
            "        x = self.bn3(self.conv3(x))\n",
            "        if self.use_stem:\n",
            "            # downsample skip connection\n",
            "            shortcut = self.conv_stem(shortcut)\n",
            "\n",
            "        # add in skip connection\n",
            "        x += shortcut\n",
            "        return self.relu(x)\n",
            "\n",
            "\n",
            "class StemConfig:\n",
            "    \"\"\"\n",
            "    convenience class to encapsulate configuration options\n",
            "    for the ResNet stem\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, num_channels, kernel_size, stride, padding):\n",
            "        self.num_channels = num_channels\n",
            "        self.kernel_size = kernel_size\n",
            "        self.stride = stride\n",
            "        self.padding = padding\n",
            "\n",
            "\n",
            "def generate_block(\n",
            "    block_type: ResidualBlockType,\n",
            "    num_channels: int,\n",
            "    use_stem: bool = False,\n",
            "    strides: int = 1,\n",
            "    factor: int = 4,\n",
            "    dropout: Optional[float] = None,\n",
            "    use_bias: bool = False,\n",
            "    main_block_kernel_size: int = 3\n",
            "):\n",
            "    \"\"\"\n",
            "    Returns either a Residual Block or a ResidualBottleneck\n",
            "    \"\"\"\n",
            "    if block_type == ResidualBlockType.BASIC:\n",
            "        return ResidualBlock(\n",
            "            num_channels,\n",
            "            use_stem=use_stem,\n",
            "            strides=strides,\n",
            "            dropout=dropout,\n",
            "            use_bias=use_bias,\n",
            "            main_block_kernel_size=main_block_kernel_size\n",
            "        )\n",
            "    else:\n",
            "        return BottleneckResidualBlock(\n",
            "            num_channels,\n",
            "            use_stem=use_stem,\n",
            "            strides=strides,\n",
            "            factor=factor,\n",
            "            dropout=dropout,\n",
            "            use_bias=use_bias,\n",
            "            main_block_kernel_size=main_block_kernel_size\n",
            "        )\n",
            "\n",
            "\n",
            "class ResNet(nn.Module):\n",
            "    \"\"\"\n",
            "    Class representing a full ResNet model\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        architecture: List[Tuple[ResidualBlockType, int, int, float, int]],\n",
            "        stem_config: Optional[StemConfig],\n",
            "        output_size: int = 10,\n",
            "        use_bias: bool = False,\n",
            "        *args,\n",
            "        **kwargs,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        returns an instance of a ResNet\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.use_bias = use_bias\n",
            "        if stem_config is not None:\n",
            "            self.stem = self.create_stem(\n",
            "                stem_config.num_channels,\n",
            "                stem_config.kernel_size,\n",
            "                stem_config.stride,\n",
            "                stem_config.padding,\n",
            "                use_bias=use_bias,\n",
            "            )\n",
            "        else:\n",
            "            self.stem = self.create_stem(use_bias=use_bias)\n",
            "        self.classifier = self.create_classifier(output_size, use_bias=use_bias)\n",
            "\n",
            "        self.body = nn.Sequential()\n",
            "        for idx, block_def in enumerate(architecture):\n",
            "            self.body.add_module(\n",
            "                f\"block_{idx+2}\",\n",
            "                self.create_block(\n",
            "                    *block_def, first_block=(idx == 0), use_bias=use_bias\n",
            "                ),\n",
            "            )\n",
            "\n",
            "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
            "        \"\"\"\n",
            "        Performs forward pass of the inputs through the network\n",
            "        \"\"\"\n",
            "        x = self.stem(inputs)\n",
            "        x = self.body(x)\n",
            "        return self.classifier(x)\n",
            "\n",
            "    def create_stem(\n",
            "        self,\n",
            "        num_channels: int = 64,\n",
            "        kernel_size: int = 7,\n",
            "        stride: int = 2,\n",
            "        padding: int = 3,\n",
            "        use_bias: bool = False,\n",
            "    ) -> nn.Sequential:\n",
            "        \"\"\"\n",
            "        Creates a sequential stem as the first component of the model\n",
            "        \"\"\"\n",
            "        return nn.Sequential(\n",
            "            nn.LazyConv2d(\n",
            "                num_channels,\n",
            "                kernel_size=kernel_size,\n",
            "                padding=padding,\n",
            "                stride=stride,\n",
            "                bias=use_bias,\n",
            "            ),\n",
            "            nn.LazyBatchNorm2d(),\n",
            "            nn.ReLU(inplace=True),\n",
            "        )\n",
            "\n",
            "    def create_classifier(\n",
            "        self, num_classes: int, use_bias: bool = False\n",
            "    ) -> nn.Sequential:\n",
            "        \"\"\"\n",
            "        Creates a sequential classifier head at the very\n",
            "        \"\"\"\n",
            "        return nn.Sequential(\n",
            "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.LazyLinear(num_classes)\n",
            "        )\n",
            "\n",
            "    def create_block(\n",
            "        self,\n",
            "        block_type: ResidualBlockType,\n",
            "        num_residuals: int,\n",
            "        num_channels: int,\n",
            "        dropout: Optional[float] = None,\n",
            "        main_block_kernel_size: int = 3,\n",
            "        first_block: bool = False,\n",
            "        use_bias: bool = False,\n",
            "    ) -> nn.Sequential:\n",
            "        \"\"\"\n",
            "        Given our inputs, generates either a ResidualBlock or ResidualBottleNeck and addes it to our\n",
            "        sequence of layers\n",
            "        \"\"\"\n",
            "        layer = []\n",
            "        for i in range(num_residuals):\n",
            "            if i == 0 and not first_block:\n",
            "                layer.append(\n",
            "                    generate_block(\n",
            "                        block_type,\n",
            "                        num_channels,\n",
            "                        use_stem=True,\n",
            "                        strides=2,\n",
            "                        dropout=dropout,\n",
            "                        use_bias=use_bias,\n",
            "                        main_block_kernel_size=main_block_kernel_size\n",
            "                    )\n",
            "                )\n",
            "            else:\n",
            "                layer.append(\n",
            "                    generate_block(\n",
            "                        block_type, num_channels, dropout=dropout, use_bias=use_bias, \n",
            "                        main_block_kernel_size=main_block_kernel_size\n",
            "                    )\n",
            "                )\n",
            "        return nn.Sequential(*layer)\n"
          ]
        }
      ],
      "source": [
        "!cat src/model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geVocFZngwER"
      },
      "source": [
        "# Import, Seed, Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Yk6T4FzhgvGv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-QebqUCUgyc-"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lsMOOo-gzTu",
        "outputId": "3c70d0d0-7e0a-4fe4-f38c-6e3389bbcf8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br5Rha2PhBoT"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD28XJ_qg1Ee",
        "outputId": "799a27ff-322f-41f2-a1ef-9d2ed1441bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from src.data import get_transformed_data, make_data_loaders\n",
        "from src.transforms import make_auto_transforms # used to use: make_transforms\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "VALID_RATIO = 0.1\n",
        "\n",
        "train_data, valid_data, test_data = \\\n",
        "get_transformed_data(make_transforms=make_auto_transforms, valid_ratio=VALID_RATIO)\n",
        "\n",
        "train_iter, valid_iter, test_iter = \\\n",
        "make_data_loaders(train_data, valid_data, test_data, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P0KpocliyKi"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09YbNCaCYxMS"
      },
      "source": [
        "## Model: import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jCSim-9qY1V4"
      },
      "outputs": [],
      "source": [
        "from src.model import StemConfig, ResidualBlockType, ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrDIWjVaY2PV"
      },
      "source": [
        "## Model: try code here\n",
        "`config_kernel_size_on_updated_main` branch > src > model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "F10Lm95gY5sG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftpl9IQRYpEX"
      },
      "source": [
        "## Architecture <<<"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ORrGNYQGix9m"
      },
      "outputs": [],
      "source": [
        "DROPOUT = 0.1\n",
        "MAIN_BLOCK_KERNEL_SIZE = 2\n",
        "\n",
        "# 1 tuple == 1 layer\n",
        "# block with or without bottleneck, how many blocks in each layer, out_channels, dropout prob, kernel_size\n",
        "architecture = [\n",
        "    (ResidualBlockType.BASIC, 2,  64, DROPOUT, MAIN_BLOCK_KERNEL_SIZE),\n",
        "    (ResidualBlockType.BASIC, 2, 128, DROPOUT, MAIN_BLOCK_KERNEL_SIZE),\n",
        "    (ResidualBlockType.BASIC, 2, 256, DROPOUT, MAIN_BLOCK_KERNEL_SIZE),\n",
        "    (ResidualBlockType.BASIC, 2, 512, DROPOUT, MAIN_BLOCK_KERNEL_SIZE),\n",
        "]\n",
        "\n",
        "# 2022.11/18(5)_p03.59 (Oscar)\n",
        "# slightly reduce the last layer's out_channels to keep overall params under 5M:\n",
        "# 512 -> 5,069,130\n",
        "# 508 -> 5,014,978\n",
        "# 507 -> 5,001,500\n",
        "# 506 -> 4,988,046 <<<"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz6S1KmzOjda",
        "outputId": "1176e9ad-1d5f-4b88-a0a8-bc41d6f05c78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "config = StemConfig(num_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "model  = ResNet(architecture, stem_config=config, output_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LwVNRxIUnAJ"
      },
      "source": [
        "## Initialize model shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Dw4mJFmkKu",
        "outputId": "37a4a8ce-e27e-4a0b-8772-0bea1fb60913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------\n",
            "-------------------\n",
            "ResNet(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            "  (body): Sequential(\n",
            "    (block_2): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "    (block_3): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "    (block_4): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "    (block_5): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (conv1): Sequential(\n",
            "          (0): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (1): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
            "        )\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "(5069130, 5069130)\n",
            "torch.Size([128, 10])\n"
          ]
        }
      ],
      "source": [
        "# intialize a new model\n",
        "\n",
        "inputs = torch.empty((BATCH_SIZE, 3, 32, 32)) #  passed\n",
        "inputs.normal_()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "outputs = model(inputs.to(device)) # internally converts all nn.LazyConv2d layers to nn.Conv2d\n",
        "\n",
        "print('-------------------')\n",
        "print('-------------------')\n",
        "\n",
        "print(model)\n",
        "\n",
        "from src.utils import count_parameters\n",
        "\n",
        "print(count_parameters(model))\n",
        "print(outputs.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWvfEz42UfD0"
      },
      "source": [
        "## Count parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJeHP8QWT_mQ",
        "outputId": "89ef0fbf-fa85-4078-8de1-69b5bf300d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5069130\n"
          ]
        }
      ],
      "source": [
        "num_parameters, num_parameters_requiring_grad = count_parameters(model)\n",
        "print(num_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuhd85NQdSVI",
        "outputId": "64a8c1f9-0e35-4082-c979-a52ca16ab232"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Sequential: 1-1                        [-1, 64, 32, 32]          --\n",
              "|    Conv2d: 2-1                       [-1, 64, 32, 32]          1,728\n",
              "|    BatchNorm2d: 2-2                  [-1, 64, 32, 32]          128\n",
              "|    ReLU: 2-3                         [-1, 64, 32, 32]          --\n",
              "Sequential: 1-2                        [-1, 512, 4, 4]           --\n",
              "|    Sequential: 2-4                   [-1, 64, 32, 32]          --\n",
              "|    |    ResidualBlock: 3-1           [-1, 64, 32, 32]          33,024\n",
              "|    |    ResidualBlock: 3-2           [-1, 64, 32, 32]          33,024\n",
              "|    Sequential: 2-5                   [-1, 128, 16, 16]         --\n",
              "|    |    ResidualBlock: 3-3           [-1, 128, 16, 16]         107,008\n",
              "|    |    ResidualBlock: 3-4           [-1, 128, 16, 16]         131,584\n",
              "|    Sequential: 2-6                   [-1, 256, 8, 8]           --\n",
              "|    |    ResidualBlock: 3-5           [-1, 256, 8, 8]           427,008\n",
              "|    |    ResidualBlock: 3-6           [-1, 256, 8, 8]           525,312\n",
              "|    Sequential: 2-7                   [-1, 512, 4, 4]           --\n",
              "|    |    ResidualBlock: 3-7           [-1, 512, 4, 4]           1,705,984\n",
              "|    |    ResidualBlock: 3-8           [-1, 512, 4, 4]           2,099,200\n",
              "Sequential: 1-3                        [-1, 10]                  --\n",
              "|    AdaptiveAvgPool2d: 2-8            [-1, 512, 1, 1]           --\n",
              "|    Flatten: 2-9                      [-1, 512]                 --\n",
              "|    Linear: 2-10                      [-1, 10]                  5,130\n",
              "==========================================================================================\n",
              "Total params: 5,069,130\n",
              "Trainable params: 5,069,130\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 28.13\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 5.19\n",
              "Params size (MB): 19.34\n",
              "Estimated Total Size (MB): 24.54\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "QUIET_VERBOSE = 0\n",
        "\n",
        "one_sample_input_shape = (3, 32, 32)\n",
        "\n",
        "summary(model, one_sample_input_shape, verbose = QUIET_VERBOSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQKBIE8_UgoP"
      },
      "source": [
        "## Initialize parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uXBP9Cm0Up-h"
      },
      "outputs": [],
      "source": [
        "from src.utils import initialize_parameters\n",
        "\n",
        "# initialize model weights\n",
        "model.apply(initialize_parameters);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xAeC3JkTZPn"
      },
      "source": [
        "# Train, validate, log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbTgjcLMYpEa"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SNYcxMz6YpEa"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RticcmCFYpEa"
      },
      "source": [
        "## Optimizer, scheduler (learning rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oOacAoIeYpEb"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "\n",
        "learning_rate = 0.01\n",
        "max_lr        = 0.5\n",
        "base_lr       = 0.001\n",
        "\n",
        "lr_schedule   = 'CyclicLR_base_0.001_max_0.5'\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=learning_rate, cycle_momentum=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSDlhXutYpEb"
      },
      "source": [
        "## Iterate!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "G8RjsKs0ZjgD"
      },
      "outputs": [],
      "source": [
        "from src.engine import train_one_epoch, evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gvkyK2UgYpEb"
      },
      "outputs": [],
      "source": [
        "EPOCHS  = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwTSlzxwTwLf",
        "outputId": "dcddbfbe-2974-4434-e2ab-9e392f383d2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'architecture': [(<ResidualBlockType.BASIC: 0>, 2, 64, 0.1, 2),\n",
              "  (<ResidualBlockType.BASIC: 0>, 2, 128, 0.1, 2),\n",
              "  (<ResidualBlockType.BASIC: 0>, 2, 256, 0.1, 2),\n",
              "  (<ResidualBlockType.BASIC: 0>, 2, 512, 0.1, 2)],\n",
              " 'num_params': 5069130,\n",
              " 'grad_params': 5069130,\n",
              " 'main_block_kernel_size': 2,\n",
              " 'dropout': 0.1,\n",
              " 'learning_rate': 0.01,\n",
              " 'lr_schedule': 'CyclicLR_base_0.001_max_0.5',\n",
              " 'batch_size': 128,\n",
              " 'epochs': 100}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## setup wandb logging\n",
        "\n",
        "assert 'wandb' in locals() # 'wandb' has been imported already\n",
        "\n",
        "ENTITY_NAME  = 'dlf22_mini_project' # this is our team name\n",
        "\n",
        "# PROJECT_NAME = 'Junk_Test_Project'\n",
        "PROJECT_NAME = 'ResNet_5M'\n",
        "\n",
        "# RUN_NAME     = 'resnet_osca_kernel_size_2_2022.11.18.p04.13'\n",
        "RUN_NAME = 'osca_kernel_size_2_avgpool_replicate_resnet18_adamw_0.001baselr'\n",
        "\n",
        "WANDB_CONFIG = \\\n",
        "{\"architecture\"          : architecture,                 # the model\n",
        " \"num_params\"            : num_parameters,                # the model\n",
        " \"grad_params\"           : num_parameters_requiring_grad, # the model\n",
        " \"main_block_kernel_size\": MAIN_BLOCK_KERNEL_SIZE,        # the model\n",
        " \"dropout\"       : DROPOUT,         # the model\n",
        " \"learning_rate\" : learning_rate,   # how to gradient descent\n",
        " \"lr_schedule\"   : lr_schedule,     # how to gradient descent\n",
        " \"batch_size\"    : BATCH_SIZE,      # how to gradient descent\n",
        " \"epochs\"        : EPOCHS,}         # train for how long\n",
        "\n",
        "WANDB_CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "hAHgrLh_YpEc",
        "outputId": "d3870c86-c18d-41e8-e82e-19555daab888"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mos2000os\u001b[0m (\u001b[33mdlf22_mini_project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221119_201406-35ulnip8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/dlf22_mini_project/ResNet_5M/runs/35ulnip8\" target=\"_blank\">osca_kernel_size_2_avgpool_replicate_resnet18_adamw_0.001baselr</a></strong> to <a href=\"https://wandb.ai/dlf22_mini_project/ResNet_5M\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dlf22_mini_project/ResNet_5M/runs/35ulnip8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f63d02b4fd0>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb\\\n",
        ".init(name=RUN_NAME, project=PROJECT_NAME, entity=ENTITY_NAME,config=WANDB_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMyGRL1GUGXH",
        "outputId": "00e8181e-9e2f-4374-e229-44b13f079b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "osca_kernel_size_2_avgpool_replicate_resnet18_adamw_0.001baselr.pt\n"
          ]
        }
      ],
      "source": [
        "# save to disk the \"best\" model === the model with the lowest validation loss\n",
        "best_model_path = RUN_NAME + '.pt'\n",
        "print(best_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWjFOiMULJ3",
        "outputId": "6cdfb0b1-ba8f-43ac-9424-524f46200349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "\tTrain elapsed: 1:5, loss: 1.9334, acc: 34.45%\n",
            "\tValidation elapsed: 0:1, loss: 1.3580, acc: 51.68%\n",
            "Epoch 2\n",
            "\tTrain elapsed: 1:6, loss: 1.4202, acc: 49.12%\n",
            "\tValidation elapsed: 0:1, loss: 1.0738, acc: 60.41%\n",
            "Epoch 3\n",
            "\tTrain elapsed: 1:7, loss: 1.2153, acc: 56.85%\n",
            "\tValidation elapsed: 0:1, loss: 0.9568, acc: 66.76%\n",
            "Epoch 4\n",
            "\tTrain elapsed: 1:7, loss: 1.0635, acc: 62.40%\n",
            "\tValidation elapsed: 0:2, loss: 0.8450, acc: 71.21%\n",
            "Epoch 5\n",
            "\tTrain elapsed: 1:7, loss: 0.9666, acc: 66.19%\n",
            "\tValidation elapsed: 0:1, loss: 0.8336, acc: 71.72%\n",
            "Epoch 6\n",
            "\tTrain elapsed: 1:7, loss: 0.8919, acc: 68.71%\n",
            "\tValidation elapsed: 0:1, loss: 0.8168, acc: 72.21%\n",
            "Epoch 7\n",
            "\tTrain elapsed: 1:8, loss: 0.8237, acc: 71.24%\n",
            "\tValidation elapsed: 0:1, loss: 0.7161, acc: 75.82%\n",
            "Epoch 8\n",
            "\tTrain elapsed: 1:7, loss: 0.7755, acc: 72.82%\n",
            "\tValidation elapsed: 0:1, loss: 0.7044, acc: 75.53%\n",
            "Epoch 9\n",
            "\tTrain elapsed: 1:7, loss: 0.7353, acc: 74.48%\n",
            "\tValidation elapsed: 0:1, loss: 0.6729, acc: 77.29%\n",
            "Epoch 10\n",
            "\tTrain elapsed: 1:7, loss: 0.6962, acc: 75.82%\n",
            "\tValidation elapsed: 0:2, loss: 0.6086, acc: 79.32%\n",
            "Epoch 11\n",
            "\tTrain elapsed: 1:7, loss: 0.6649, acc: 76.97%\n",
            "\tValidation elapsed: 0:2, loss: 0.6282, acc: 78.63%\n",
            "Epoch 12\n",
            "\tTrain elapsed: 1:8, loss: 0.6295, acc: 78.20%\n",
            "\tValidation elapsed: 0:1, loss: 0.6408, acc: 79.38%\n",
            "Epoch 13\n",
            "\tTrain elapsed: 1:7, loss: 0.6004, acc: 79.13%\n",
            "\tValidation elapsed: 0:1, loss: 0.5673, acc: 81.52%\n",
            "Epoch 14\n",
            "\tTrain elapsed: 1:7, loss: 0.5737, acc: 80.12%\n",
            "\tValidation elapsed: 0:1, loss: 0.6161, acc: 78.93%\n",
            "Epoch 15\n",
            "\tTrain elapsed: 1:7, loss: 0.5441, acc: 81.08%\n",
            "\tValidation elapsed: 0:1, loss: 0.5245, acc: 82.50%\n",
            "Epoch 16\n",
            "\tTrain elapsed: 1:7, loss: 0.5285, acc: 81.62%\n",
            "\tValidation elapsed: 0:1, loss: 0.5769, acc: 81.07%\n",
            "Epoch 17\n",
            "\tTrain elapsed: 1:7, loss: 0.5024, acc: 82.59%\n",
            "\tValidation elapsed: 0:1, loss: 0.5047, acc: 83.54%\n",
            "Epoch 18\n",
            "\tTrain elapsed: 1:7, loss: 0.4854, acc: 82.98%\n",
            "\tValidation elapsed: 0:1, loss: 0.5042, acc: 83.65%\n",
            "Epoch 19\n",
            "\tTrain elapsed: 1:7, loss: 0.4733, acc: 83.51%\n",
            "\tValidation elapsed: 0:1, loss: 0.5367, acc: 83.01%\n",
            "Epoch 20\n",
            "\tTrain elapsed: 1:7, loss: 0.4536, acc: 84.25%\n",
            "\tValidation elapsed: 0:1, loss: 0.4841, acc: 84.08%\n",
            "Epoch 21\n",
            "\tTrain elapsed: 1:7, loss: 0.4351, acc: 84.90%\n",
            "\tValidation elapsed: 0:1, loss: 0.5010, acc: 83.75%\n",
            "Epoch 22\n",
            "\tTrain elapsed: 1:7, loss: 0.4172, acc: 85.40%\n",
            "\tValidation elapsed: 0:1, loss: 0.5047, acc: 83.93%\n",
            "Epoch 23\n",
            "\tTrain elapsed: 1:7, loss: 0.3996, acc: 85.95%\n",
            "\tValidation elapsed: 0:1, loss: 0.5063, acc: 84.55%\n",
            "Epoch 24\n",
            "\tTrain elapsed: 1:7, loss: 0.3983, acc: 86.26%\n",
            "\tValidation elapsed: 0:1, loss: 0.5310, acc: 84.18%\n",
            "Epoch 25\n",
            "\tTrain elapsed: 1:7, loss: 0.3761, acc: 87.10%\n",
            "\tValidation elapsed: 0:1, loss: 0.5499, acc: 84.20%\n",
            "Epoch 26\n",
            "\tTrain elapsed: 1:7, loss: 0.3772, acc: 87.02%\n",
            "\tValidation elapsed: 0:1, loss: 0.5015, acc: 83.98%\n",
            "Epoch 27\n",
            "\tTrain elapsed: 1:7, loss: 0.3551, acc: 87.74%\n",
            "\tValidation elapsed: 0:1, loss: 0.5172, acc: 84.84%\n",
            "Epoch 28\n",
            "\tTrain elapsed: 1:6, loss: 0.3490, acc: 87.97%\n",
            "\tValidation elapsed: 0:1, loss: 0.5526, acc: 84.26%\n",
            "Epoch 29\n",
            "\tTrain elapsed: 1:7, loss: 0.3441, acc: 88.08%\n",
            "\tValidation elapsed: 0:1, loss: 0.5774, acc: 84.51%\n",
            "Epoch 30\n",
            "\tTrain elapsed: 1:7, loss: 0.3299, acc: 88.64%\n",
            "\tValidation elapsed: 0:1, loss: 0.5431, acc: 85.00%\n",
            "Epoch 31\n",
            "\tTrain elapsed: 1:7, loss: 0.3215, acc: 88.90%\n",
            "\tValidation elapsed: 0:1, loss: 0.4819, acc: 85.43%\n",
            "Epoch 32\n",
            "\tTrain elapsed: 1:7, loss: 0.3107, acc: 89.37%\n",
            "\tValidation elapsed: 0:1, loss: 0.5148, acc: 84.73%\n",
            "Epoch 33\n",
            "\tTrain elapsed: 1:7, loss: 0.3047, acc: 89.56%\n",
            "\tValidation elapsed: 0:1, loss: 0.5390, acc: 84.88%\n",
            "Epoch 34\n",
            "\tTrain elapsed: 1:7, loss: 0.3005, acc: 89.69%\n",
            "\tValidation elapsed: 0:1, loss: 0.5824, acc: 83.87%\n",
            "Epoch 35\n",
            "\tTrain elapsed: 1:7, loss: 0.2975, acc: 89.87%\n",
            "\tValidation elapsed: 0:1, loss: 0.5513, acc: 84.75%\n",
            "Epoch 36\n",
            "\tTrain elapsed: 1:6, loss: 0.2929, acc: 89.85%\n",
            "\tValidation elapsed: 0:1, loss: 0.5507, acc: 84.75%\n",
            "Epoch 37\n",
            "\tTrain elapsed: 1:7, loss: 0.2893, acc: 89.94%\n",
            "\tValidation elapsed: 0:1, loss: 0.5764, acc: 84.47%\n",
            "Epoch 38\n",
            "\tTrain elapsed: 1:6, loss: 0.2845, acc: 90.24%\n",
            "\tValidation elapsed: 0:1, loss: 0.5494, acc: 84.65%\n",
            "Epoch 39\n",
            "\tTrain elapsed: 1:6, loss: 0.2768, acc: 90.46%\n",
            "\tValidation elapsed: 0:1, loss: 0.5045, acc: 85.49%\n",
            "Epoch 40\n",
            "\tTrain elapsed: 1:6, loss: 0.2764, acc: 90.56%\n",
            "\tValidation elapsed: 0:1, loss: 0.4949, acc: 85.57%\n",
            "Epoch 41\n",
            "\tTrain elapsed: 1:7, loss: 0.2775, acc: 90.71%\n",
            "\tValidation elapsed: 0:2, loss: 0.5632, acc: 85.14%\n",
            "Epoch 42\n",
            "\tTrain elapsed: 1:7, loss: 0.2631, acc: 90.99%\n",
            "\tValidation elapsed: 0:1, loss: 0.5615, acc: 85.45%\n",
            "Epoch 43\n",
            "\tTrain elapsed: 1:7, loss: 0.2609, acc: 91.08%\n",
            "\tValidation elapsed: 0:2, loss: 0.5052, acc: 85.35%\n",
            "Epoch 44\n",
            "\tTrain elapsed: 1:7, loss: 0.2570, acc: 91.22%\n",
            "\tValidation elapsed: 0:1, loss: 0.5199, acc: 85.59%\n",
            "Epoch 45\n",
            "\tTrain elapsed: 1:7, loss: 0.2546, acc: 91.28%\n",
            "\tValidation elapsed: 0:1, loss: 0.5481, acc: 85.59%\n",
            "Epoch 46\n",
            "\tTrain elapsed: 1:7, loss: 0.2552, acc: 91.21%\n",
            "\tValidation elapsed: 0:1, loss: 0.4485, acc: 86.39%\n",
            "Epoch 47\n",
            "\tTrain elapsed: 1:7, loss: 0.2431, acc: 91.79%\n",
            "\tValidation elapsed: 0:1, loss: 0.5314, acc: 85.55%\n",
            "Epoch 48\n",
            "\tTrain elapsed: 1:6, loss: 0.2379, acc: 91.80%\n",
            "\tValidation elapsed: 0:1, loss: 0.5124, acc: 86.02%\n",
            "Epoch 49\n",
            "\tTrain elapsed: 1:6, loss: 0.2386, acc: 91.93%\n",
            "\tValidation elapsed: 0:1, loss: 0.5445, acc: 85.45%\n",
            "Epoch 50\n",
            "\tTrain elapsed: 1:6, loss: 0.2485, acc: 91.63%\n",
            "\tValidation elapsed: 0:1, loss: 0.5341, acc: 86.05%\n",
            "Epoch 51\n",
            "\tTrain elapsed: 1:6, loss: 0.2467, acc: 91.53%\n",
            "\tValidation elapsed: 0:1, loss: 0.5343, acc: 85.70%\n",
            "Epoch 52\n",
            "\tTrain elapsed: 1:6, loss: 0.2411, acc: 91.63%\n",
            "\tValidation elapsed: 0:1, loss: 0.5466, acc: 86.21%\n",
            "Epoch 53\n",
            "\tTrain elapsed: 1:6, loss: 0.2279, acc: 92.27%\n",
            "\tValidation elapsed: 0:1, loss: 0.5899, acc: 85.59%\n",
            "Epoch 54\n",
            "\tTrain elapsed: 1:6, loss: 0.2270, acc: 92.12%\n",
            "\tValidation elapsed: 0:1, loss: 0.5291, acc: 86.11%\n",
            "Epoch 55\n",
            "\tTrain elapsed: 1:6, loss: 0.2323, acc: 92.05%\n",
            "\tValidation elapsed: 0:1, loss: 0.5304, acc: 86.23%\n",
            "Epoch 56\n",
            "\tTrain elapsed: 1:6, loss: 0.2228, acc: 92.37%\n",
            "\tValidation elapsed: 0:1, loss: 0.5538, acc: 85.33%\n",
            "Epoch 57\n",
            "\tTrain elapsed: 1:6, loss: 0.2363, acc: 91.98%\n",
            "\tValidation elapsed: 0:1, loss: 0.5031, acc: 85.35%\n",
            "Epoch 58\n",
            "\tTrain elapsed: 1:6, loss: 0.2196, acc: 92.55%\n",
            "\tValidation elapsed: 0:1, loss: 0.4892, acc: 86.68%\n",
            "Epoch 59\n",
            "\tTrain elapsed: 1:6, loss: 0.2180, acc: 92.61%\n",
            "\tValidation elapsed: 0:1, loss: 0.5880, acc: 84.92%\n",
            "Epoch 60\n",
            "\tTrain elapsed: 1:6, loss: 0.2184, acc: 92.64%\n",
            "\tValidation elapsed: 0:1, loss: 0.5297, acc: 85.55%\n",
            "Epoch 61\n",
            "\tTrain elapsed: 1:6, loss: 0.2187, acc: 92.59%\n",
            "\tValidation elapsed: 0:1, loss: 0.4756, acc: 86.78%\n",
            "Epoch 62\n",
            "\tTrain elapsed: 1:6, loss: 0.2170, acc: 92.58%\n",
            "\tValidation elapsed: 0:1, loss: 0.4550, acc: 86.76%\n",
            "Epoch 63\n",
            "\tTrain elapsed: 1:6, loss: 0.2130, acc: 92.64%\n",
            "\tValidation elapsed: 0:1, loss: 0.5448, acc: 86.82%\n",
            "Epoch 64\n",
            "\tTrain elapsed: 1:6, loss: 0.2163, acc: 92.58%\n",
            "\tValidation elapsed: 0:1, loss: 0.5211, acc: 85.98%\n",
            "Epoch 65\n",
            "\tTrain elapsed: 1:6, loss: 0.2072, acc: 92.86%\n",
            "\tValidation elapsed: 0:1, loss: 0.5528, acc: 85.90%\n",
            "Epoch 66\n",
            "\tTrain elapsed: 1:6, loss: 0.2150, acc: 92.77%\n",
            "\tValidation elapsed: 0:1, loss: 0.4928, acc: 87.01%\n",
            "Epoch 67\n",
            "\tTrain elapsed: 1:6, loss: 0.2114, acc: 92.80%\n",
            "\tValidation elapsed: 0:1, loss: 0.5620, acc: 85.61%\n",
            "Epoch 68\n",
            "\tTrain elapsed: 1:6, loss: 0.2100, acc: 92.89%\n",
            "\tValidation elapsed: 0:1, loss: 0.5142, acc: 87.56%\n",
            "Epoch 69\n",
            "\tTrain elapsed: 1:6, loss: 0.2091, acc: 92.89%\n",
            "\tValidation elapsed: 0:2, loss: 0.5665, acc: 86.58%\n",
            "Epoch 70\n",
            "\tTrain elapsed: 1:6, loss: 0.2065, acc: 92.86%\n",
            "\tValidation elapsed: 0:1, loss: 0.5681, acc: 86.11%\n",
            "Epoch 71\n",
            "\tTrain elapsed: 1:6, loss: 0.2002, acc: 93.19%\n",
            "\tValidation elapsed: 0:1, loss: 0.5652, acc: 86.07%\n",
            "Epoch 72\n",
            "\tTrain elapsed: 1:6, loss: 0.2084, acc: 92.85%\n",
            "\tValidation elapsed: 0:1, loss: 0.5645, acc: 85.66%\n",
            "Epoch 73\n",
            "\tTrain elapsed: 1:5, loss: 0.2019, acc: 93.21%\n",
            "\tValidation elapsed: 0:1, loss: 0.5643, acc: 85.88%\n",
            "Epoch 74\n",
            "\tTrain elapsed: 1:6, loss: 0.1975, acc: 93.25%\n",
            "\tValidation elapsed: 0:1, loss: 0.5206, acc: 85.74%\n",
            "Epoch 75\n",
            "\tTrain elapsed: 1:6, loss: 0.2016, acc: 93.21%\n",
            "\tValidation elapsed: 0:1, loss: 0.5616, acc: 86.04%\n",
            "Epoch 76\n",
            "\tTrain elapsed: 1:6, loss: 0.1979, acc: 93.17%\n",
            "\tValidation elapsed: 0:1, loss: 0.5330, acc: 86.70%\n",
            "Epoch 77\n",
            "\tTrain elapsed: 1:6, loss: 0.1953, acc: 93.37%\n",
            "\tValidation elapsed: 0:1, loss: 0.5411, acc: 86.35%\n",
            "Epoch 78\n",
            "\tTrain elapsed: 1:6, loss: 0.1976, acc: 93.29%\n",
            "\tValidation elapsed: 0:1, loss: 0.5461, acc: 86.25%\n",
            "Epoch 79\n",
            "\tTrain elapsed: 1:6, loss: 0.1932, acc: 93.52%\n",
            "\tValidation elapsed: 0:1, loss: 0.5584, acc: 86.13%\n",
            "Epoch 80\n",
            "\tTrain elapsed: 1:6, loss: 0.1953, acc: 93.36%\n",
            "\tValidation elapsed: 0:1, loss: 0.5774, acc: 86.37%\n",
            "Epoch 81\n",
            "\tTrain elapsed: 1:6, loss: 0.1940, acc: 93.28%\n",
            "\tValidation elapsed: 0:1, loss: 0.5402, acc: 86.66%\n",
            "Epoch 82\n",
            "\tTrain elapsed: 1:6, loss: 0.1971, acc: 93.19%\n",
            "\tValidation elapsed: 0:1, loss: 0.5459, acc: 86.72%\n",
            "Epoch 83\n",
            "\tTrain elapsed: 1:6, loss: 0.1930, acc: 93.43%\n",
            "\tValidation elapsed: 0:1, loss: 0.5499, acc: 85.88%\n",
            "Epoch 84\n",
            "\tTrain elapsed: 1:6, loss: 0.1911, acc: 93.50%\n",
            "\tValidation elapsed: 0:1, loss: 0.5579, acc: 85.82%\n",
            "Epoch 85\n",
            "\tTrain elapsed: 1:6, loss: 0.1896, acc: 93.54%\n",
            "\tValidation elapsed: 0:1, loss: 0.5456, acc: 86.39%\n",
            "Epoch 86\n",
            "\tTrain elapsed: 1:6, loss: 0.1855, acc: 93.68%\n",
            "\tValidation elapsed: 0:1, loss: 0.5718, acc: 85.80%\n",
            "Epoch 87\n",
            "\tTrain elapsed: 1:6, loss: 0.1853, acc: 93.60%\n",
            "\tValidation elapsed: 0:1, loss: 0.5641, acc: 86.58%\n",
            "Epoch 88\n",
            "\tTrain elapsed: 1:6, loss: 0.1918, acc: 93.47%\n",
            "\tValidation elapsed: 0:1, loss: 0.5285, acc: 86.48%\n",
            "Epoch 89\n",
            "\tTrain elapsed: 1:6, loss: 0.1879, acc: 93.52%\n",
            "\tValidation elapsed: 0:1, loss: 0.5828, acc: 86.09%\n",
            "Epoch 90\n",
            "\tTrain elapsed: 1:6, loss: 0.1862, acc: 93.74%\n",
            "\tValidation elapsed: 0:1, loss: 0.5049, acc: 87.11%\n",
            "Epoch 91\n",
            "\tTrain elapsed: 1:6, loss: 0.1826, acc: 93.76%\n",
            "\tValidation elapsed: 0:1, loss: 0.5472, acc: 87.17%\n",
            "Epoch 92\n",
            "\tTrain elapsed: 1:6, loss: 0.1789, acc: 93.80%\n",
            "\tValidation elapsed: 0:1, loss: 0.5248, acc: 87.07%\n",
            "Epoch 93\n",
            "\tTrain elapsed: 1:6, loss: 0.1786, acc: 93.87%\n",
            "\tValidation elapsed: 0:1, loss: 0.5549, acc: 86.27%\n",
            "Epoch 94\n",
            "\tTrain elapsed: 1:6, loss: 0.1789, acc: 93.92%\n",
            "\tValidation elapsed: 0:1, loss: 0.5387, acc: 87.36%\n",
            "Epoch 95\n",
            "\tTrain elapsed: 1:6, loss: 0.1780, acc: 93.92%\n",
            "\tValidation elapsed: 0:1, loss: 0.5635, acc: 86.25%\n",
            "Epoch 96\n",
            "\tTrain elapsed: 1:6, loss: 0.1789, acc: 93.82%\n",
            "\tValidation elapsed: 0:1, loss: 0.5679, acc: 86.25%\n",
            "Epoch 97\n",
            "\tTrain elapsed: 1:6, loss: 0.1821, acc: 93.89%\n",
            "\tValidation elapsed: 0:1, loss: 0.5350, acc: 86.74%\n",
            "Epoch 98\n",
            "\tTrain elapsed: 1:6, loss: 0.1788, acc: 93.86%\n",
            "\tValidation elapsed: 0:1, loss: 0.5909, acc: 86.29%\n",
            "Epoch 99\n",
            "\tTrain elapsed: 1:6, loss: 0.1773, acc: 93.91%\n",
            "\tValidation elapsed: 0:1, loss: 0.5240, acc: 86.68%\n",
            "Epoch 100\n",
            "\tTrain elapsed: 1:6, loss: 0.1789, acc: 93.86%\n",
            "\tValidation elapsed: 0:1, loss: 0.5597, acc: 86.25%\n"
          ]
        }
      ],
      "source": [
        "from src.utils import epoch_time\n",
        "\n",
        "FIRST_EPOCH = 1\n",
        "\n",
        "best_loss = float('inf')\n",
        "for epoch in range(FIRST_EPOCH, FIRST_EPOCH + EPOCHS):\n",
        "\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    \n",
        "    ## TRAIN\n",
        "    start = time.time()\n",
        "    # train\n",
        "    train_loss, train_acc  = train_one_epoch(model, train_iter, criterion, optimizer, device)\n",
        "    # log & echo\n",
        "    train_mins, train_secs = epoch_time(start, time.time())\n",
        "    wandb.log({\"train_loss\": train_loss,\n",
        "               \"train_acc\" : train_acc,\n",
        "               \"epoch\"     : epoch})\n",
        "    print(f\"\\tTrain elapsed: {train_mins}:{train_secs}, loss: {train_loss:.4f}, acc: {train_acc * 100:.2f}%\")\n",
        "\n",
        "    ## VALIDATE\n",
        "    start = time.time()\n",
        "    # validate\n",
        "    val_loss, val_acc  = evaluate(model, valid_iter, criterion, device)\n",
        "    # log & echo   \n",
        "    val_mins, val_secs = epoch_time(start, time.time())\n",
        "    wandb.log({\"val_loss\": val_loss,\n",
        "               \"val_acc\" : val_acc,\n",
        "               \"epoch\"   : epoch,})\n",
        "    print(f\"\\tValidation elapsed: {val_mins}:{val_secs}, loss: {val_loss:.4f}, acc: {val_acc * 100:.2f}%\")\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    wandb.log({\"current_lr\": current_lr,\n",
        "               \"current_learning_rate\": current_lr,\n",
        "               \"epoch\"     : epoch})\n",
        "    \n",
        "    ## Memorize \"best\" model === the model with the lowest validation loss\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGr403ktYpEc"
      },
      "source": [
        "# Test, log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0myntf_YpEd",
        "outputId": "c3eda673-78ca-4b8d-9f29-fbf61533b7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "osca_kernel_size_2_avgpool_replicate_resnet18_adamw_0.001baselr.pt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(best_model_path),\n",
        "model.load_state_dict(torch.load(best_model_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuDFGLo_YpEd",
        "outputId": "fb9aaf20-4cc3-4de9-863d-9b13737a034b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.4764\n",
            "Test Accuracy: 85.89%\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = evaluate(model.to(device), test_iter, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}\\nTest Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_acc\" : test_acc,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "EXJRW1GLYpEd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.4 32-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "2e8299d72dfe468fd613ffad4039e402b7a97d3c9e4e820f9c8d9bd7574a5870"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
