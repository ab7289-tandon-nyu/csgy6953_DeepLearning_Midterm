{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO432dGb7fcH+GVGqPweXTp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ab7289-tandon-nyu/csgy6953_DeepLearning_Midterm/blob/oscar2/notebooks/oscar_test_config_kernel_size.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb Install, Login, Import"
      ],
      "metadata": {
        "id": "JEg6gGl4gdFv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrgALvYPgMBy",
        "outputId": "30fc2740-7939-4688-c75a-618031b2e662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.5)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login \"6f19b1e6735ebc69af24f18d5b426262416027fb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBNArx2fgf-v",
        "outputId": "0247c35d-a9e0-46fd-cfff-20ff427a0c2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "MP8Tcl1ogjVP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone Team's Code"
      ],
      "metadata": {
        "id": "1bbnVuzegpXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/csgy6953_DeepLearning_Midterm/"
      ],
      "metadata": {
        "id": "rSPl_TQsiRgl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b config_kernel_size \"https://github.com/ab7289-tandon-nyu/csgy6953_DeepLearning_Midterm.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw2LzDqIgoQt",
        "outputId": "43a4b090-f46d-4c47-84a3-25ffe130b970"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'csgy6953_DeepLearning_Midterm'...\n",
            "remote: Enumerating objects: 569, done.\u001b[K\n",
            "remote: Counting objects: 100% (294/294), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 569 (delta 171), reused 211 (delta 128), pack-reused 275\u001b[K\n",
            "Receiving objects: 100% (569/569), 148.54 KiB | 601.00 KiB/s, done.\n",
            "Resolving deltas: 100% (354/354), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/csgy6953_DeepLearning_Midterm/src/ ."
      ],
      "metadata": {
        "id": "17_B3oo1glfe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat src/transforms.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuMWXJYciM-S",
        "outputId": "5cbd746b-a53b-4a25-da16-2ba2fad03532"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch\n",
            "import torchvision.transforms as transforms\n",
            "from torchvision.transforms import autoaugment\n",
            "\n",
            "from typing import Tuple\n",
            "\n",
            "\n",
            "def make_transforms(means: torch.Tensor, std_devs: torch.Tensor) -> Tuple:\n",
            "    '''\n",
            "    Given a tensor of computed means and a tensor of computed standard devations,\n",
            "    return's a tuple containing a train and test transform pipelines\n",
            "    '''\n",
            "    train_transforms = transforms.Compose([\n",
            "        transforms.RandomRotation(5),\n",
            "        transforms.RandomHorizontalFlip(0.5),\n",
            "        transforms.RandomCrop(32, padding=2),\n",
            "        transforms.ToTensor(),\n",
            "        transforms.Normalize(mean=means,\n",
            "                             std=std_devs)\n",
            "    ])\n",
            "\n",
            "    test_transforms = transforms.Compose([\n",
            "        transforms.ToTensor(),\n",
            "        transforms.Normalize(mean=means,\n",
            "                             std=std_devs)\n",
            "    ])\n",
            "\n",
            "    return train_transforms, test_transforms\n",
            "\n",
            "\n",
            "def make_auto_transforms(means: torch.Tensor, std_devs: torch.Tensor) -> Tuple:\n",
            "    '''\n",
            "    Utilizes PyTorch'es AutoAugment class\n",
            "    '''\n",
            "    policy = autoaugment.AutoAugmentPolicy.CIFAR10\n",
            "\n",
            "    train_trainsforms = transforms.Compose([\n",
            "        transforms.AutoAugment(policy = policy),\n",
            "        transforms.ToTensor(),\n",
            "        transforms.Normalize(mean = means, std = std_devs)\n",
            "    ])\n",
            "\n",
            "    test_transforms = transforms.Compose([\n",
            "        transforms.ToTensor(),\n",
            "        transforms.Normalize(mean = means, std = std_devs)\n",
            "    ])\n",
            "\n",
            "    return train_trainsforms, test_transforms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import, Seed, Device"
      ],
      "metadata": {
        "id": "geVocFZngwER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "Yk6T4FzhgvGv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "-QebqUCUgyc-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lsMOOo-gzTu",
        "outputId": "de2ba376-d3e4-4c15-f903-88baffb5be1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "Br5Rha2PhBoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.data import get_transformed_data, make_data_loaders\n",
        "from src.transforms import make_auto_transforms # used to use: make_transforms\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "VALID_RATIO = 0.1\n",
        "\n",
        "train_data, valid_data, test_data = \\\n",
        "get_transformed_data(make_transforms=make_auto_transforms, valid_ratio=VALID_RATIO)\n",
        "\n",
        "train_iter, valid_iter, test_iter = \\\n",
        "make_data_loaders(train_data, valid_data, test_data, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD28XJ_qg1Ee",
        "outputId": "0f36dc1b-93c0-4751-a418-d631ba927f78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "9P0KpocliyKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    '''\n",
        "    Class representing a convolutional residual block \n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_channels: int, use_stem: bool = False, strides: int = 1, dropout: Optional[float] = None, kernel_size: int = 3):\n",
        "        '''\n",
        "        Creates a new instance of a Residual Block\n",
        "        @param: num_channels (int) - the number of output channels for all convolutions in \n",
        "            the block\n",
        "        @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the\n",
        "            residual\n",
        "        @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
        "        @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.num_channels = num_channels\n",
        "        self.use_stem = use_stem\n",
        "        self.strides = strides\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout) if dropout is not None else None\n",
        "\n",
        "        if self.kernel_size == 3:\n",
        "            self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1, stride=strides)\n",
        "            self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # partial solution 1:\n",
        "        if kernel_size == 2:\n",
        "            self.conv1 = nn.LazyConv2d(num_channels, kernel_size=2, padding=1, stride=strides)\n",
        "            self.conv2 = nn.LazyConv2d(num_channels, kernel_size=2, padding=0)\n",
        "            # fetal issue: when input is [256,1,1], MUST pad in order to apply 2x2 kernel\n",
        "\n",
        "        # # partial solution 2:\n",
        "        # if self.kernel_size == 2:\n",
        "        #     self.conv1 = nn.LazyConv2d(num_channels, kernel_size=2, padding=1, stride=strides)\n",
        "        #     self.conv2 = nn.LazyConv2d(num_channels, kernel_size=2, padding=1)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.out = nn.ReLU(inplace=True)\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "        self.conv_stem = None\n",
        "        if use_stem:\n",
        "\n",
        "            if kernel_size == 3:\n",
        "                self.conv_stem = nn.LazyConv2d(num_channels, kernel_size=1, stride=strides)\n",
        "            if kernel_size == 2:\n",
        "                self.conv_stem = nn.LazyConv2d(num_channels, kernel_size=1, stride=strides)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        print()\n",
        "        print('--------ResidualBlock:--------')\n",
        "        print(f'(use_stem={self.use_stem})')\n",
        "        print('block input:', inputs.shape)\n",
        "\n",
        "        shortcut = inputs\n",
        "\n",
        "        print()\n",
        "        print('F(x):', self.conv1)\n",
        "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
        "        print('output:', x.shape)\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        \n",
        "        print()\n",
        "        print('F(x):', self.conv2)\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        print('output:', x.shape)\n",
        "\n",
        "        if self.use_stem:\n",
        "            # downsample skip connection\n",
        "            print()\n",
        "            print('S(x):', self.conv_stem)\n",
        "            shortcut = self.conv_stem(shortcut)\n",
        "            print('output:', shortcut.shape)\n",
        "\n",
        "        # partial solution 2:\n",
        "        # if self.kernel_size == 2:\n",
        "        #     if x.shape[-1] > shortcut.shape[-1]:\n",
        "        #         x = F.pad(x, pad = (-1, -1, -1, -1))\n",
        "        #         print('negative padding =>', x.shape)\n",
        "        #         # fetal error: when F(x)'s output becomes [512,2,2] whiel input was [256,1,1], this would reduce F(x)'s output to [512,0,0]\n",
        " \n",
        "        # add in skip connection\n",
        "        x += shortcut\n",
        "\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "class StemConfig:\n",
        "    '''\n",
        "    convenience class to encapsulate configuration options\n",
        "    for the ResNet stem\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_channels, kernel_size, stride, padding):\n",
        "        self.num_channels = num_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    '''\n",
        "    Class representing a full ResNet model\n",
        "    '''\n",
        "\n",
        "    def __init__(self, architecture: List[Tuple[int, int, float]], stem_config: Optional[StemConfig], output_size: int = 10, *args, **kwargs):\n",
        "        '''\n",
        "        returns an instance of a ResNet\n",
        "        '''\n",
        "        super().__init__()\n",
        "        if stem_config is not None:\n",
        "            self.stem = self.create_stem(\n",
        "                stem_config.num_channels,\n",
        "                stem_config.kernel_size,\n",
        "                stem_config.stride,\n",
        "                stem_config.padding\n",
        "            )\n",
        "        else:\n",
        "            self.stem = self.create_stem()\n",
        "        self.classifier = self.create_classifier(output_size)\n",
        "\n",
        "        self.body = nn.Sequential()\n",
        "        for idx, block_def in enumerate(architecture):\n",
        "            self.body.add_module(\n",
        "                f\"block_{idx+2}\", self.create_block(*block_def, first_block=(idx == 0)))\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs forward pass of the inputs through the network\n",
        "        \"\"\"\n",
        "        x = self.stem(inputs)\n",
        "        x = self.body(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "    def create_stem(self, num_channels: int = 64, kernel_size: int = 7, stride: int = 2, padding: int = 3) \\\n",
        "            -> nn.Sequential:\n",
        "        \"\"\"\n",
        "        Creates a sequential stem as the first component of the model\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(num_channels, kernel_size=kernel_size,\n",
        "                          padding=padding, stride=stride),\n",
        "            nn.LazyBatchNorm2d(),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "    def create_classifier(self, num_classes: int) -> nn.Sequential:\n",
        "        '''\n",
        "        Creates a sequential classifier head at the very \n",
        "        '''\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(num_classes)\n",
        "        )\n",
        "\n",
        "    def create_block(self, num_residuals: int, num_channels: int, dropout: float, kernel_size: int, first_block: bool = False) -> nn.Sequential:\n",
        "        layer = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                layer.append(ResidualBlock(num_channels, dropout=dropout, kernel_size=kernel_size, use_stem=True, strides=2))\n",
        "            else:\n",
        "                layer.append(ResidualBlock(num_channels, dropout=dropout, kernel_size=kernel_size))\n",
        "        return nn.Sequential(*layer)\n"
      ],
      "metadata": {
        "id": "HquK6AiLDIXN"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from src.model import ResNet, StemConfig\n",
        "from src.model import StemConfig\n",
        "\n",
        "# 1 tuple == 1 layer\n",
        "# how many blocks in each layer, out_channels, dropout prob, kernel_size\n",
        "architecture = [\n",
        "    (2, 64, 0.5, 2),\n",
        "    (2, 128, 0.5, 2),\n",
        "    (2, 256, 0.5, 2),\n",
        "    (2, 512, 0.5, 2),\n",
        "]\n",
        "\n",
        "config = StemConfig(num_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "model  = ResNet(architecture, stem_config=config, output_size=10)"
      ],
      "metadata": {
        "id": "ORrGNYQGix9m"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils import count_parameters"
      ],
      "metadata": {
        "id": "HtLNSJIGUH5K"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intialize a new model\n",
        "\n",
        "# inputs = torch.empty((BATCH_SIZE, 3, 512, 512)) # passed\n",
        "inputs = torch.empty((BATCH_SIZE, 3, 32, 32)) #  passed\n",
        "# inputs = torch.empty((BATCH_SIZE, 3, 4, 4))\n",
        "\n",
        "inputs.normal_()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "outputs = model(inputs.to(device)) \n",
        "# (Oscar) observation: 2022.11/13(7)_a08.14: internally, this converts all nn.LazyConv2d layers to nn.Conv2d\n",
        "# to see this, run print(model) both before and after this operation\n",
        "\n",
        "print('-------------------')\n",
        "print('-------------------')\n",
        "\n",
        "print(model)\n",
        "print(count_parameters(model))\n",
        "print(outputs.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Dw4mJFmkKu",
        "outputId": "470be1b0-e9c9-4805-ac68-30506a0c3bfe"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------ResidualBlock:--------\n",
            "(use_stem=False)\n",
            "block input: torch.Size([4, 64, 32, 32])\n",
            "\n",
            "F(x): LazyConv2d(0, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "output: torch.Size([4, 64, 33, 33])\n",
            "\n",
            "F(x): LazyConv2d(0, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "output: torch.Size([4, 64, 32, 32])\n",
            "\n",
            "--------ResidualBlock:--------\n",
            "(use_stem=False)\n",
            "block input: torch.Size([4, 64, 32, 32])\n",
            "\n",
            "F(x): LazyConv2d(0, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "output: torch.Size([4, 64, 33, 33])\n",
            "\n",
            "F(x): LazyConv2d(0, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "output: torch.Size([4, 64, 32, 32])\n",
            "\n",
            "--------ResidualBlock:--------\n",
            "(use_stem=True)\n",
            "block input: torch.Size([4, 64, 32, 32])\n",
            "\n",
            "F(x): LazyConv2d(0, 128, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
            "output: torch.Size([4, 128, 17, 17])\n",
            "\n",
            "F(x): LazyConv2d(0, 128, kernel_size=(2, 2), stride=(1, 1))\n",
            "output: torch.Size([4, 128, 16, 16])\n",
            "\n",
            "S(x): LazyConv2d(0, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "output: torch.Size([4, 128, 16, 16])\n",
            "\n",
            "--------ResidualBlock:--------\n",
            "(use_stem=False)\n",
            "block input: torch.Size([4, 128, 16, 16])\n",
            "\n",
            "F(x): LazyConv2d(0, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "output: torch.Size([4, 128, 17, 17])\n",
            "\n",
            "F(x): LazyConv2d(0, 128, kernel_size=(2, 2), stride=(1, 1))\n",
            "output: torch.Size([4, 128, 16, 16])\n",
            "\n",
            "--------ResidualBlock:--------\n",
            "(use_stem=True)\n",
            "block input: torch.Size([4, 128, 16, 16])\n",
            "\n",
            "F(x): LazyConv2d(0, 256, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
            "output: torch.Size([4, 256, 9, 9])\n",
            "\n",
            "F(x): LazyConv2d(0, 256, kernel_size=(2, 2), stride=(1, 1))\n",
            "output: torch.Size([4, 256, 8, 8])\n",
            "\n",
            "S(x): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "output: torch.Size([4, 256, 8, 8])\n",
            "\n",
            "--------ResidualBlock:--------\n",
            "(use_stem=False)\n",
            "block input: torch.Size([4, 256, 8, 8])\n",
            "\n",
            "F(x): LazyConv2d(0, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "output: torch.Size([4, 256, 9, 9])\n",
            "\n",
            "F(x): LazyConv2d(0, 256, kernel_size=(2, 2), stride=(1, 1))\n",
            "output: torch.Size([4, 256, 8, 8])\n",
            "\n",
            "--------ResidualBlock:--------\n",
            "(use_stem=True)\n",
            "block input: torch.Size([4, 256, 8, 8])\n",
            "\n",
            "F(x): LazyConv2d(0, 512, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
            "output: torch.Size([4, 512, 5, 5])\n",
            "\n",
            "F(x): LazyConv2d(0, 512, kernel_size=(2, 2), stride=(1, 1))\n",
            "output: torch.Size([4, 512, 4, 4])\n",
            "\n",
            "S(x): LazyConv2d(0, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "output: torch.Size([4, 512, 4, 4])\n",
            "\n",
            "--------ResidualBlock:--------\n",
            "(use_stem=False)\n",
            "block input: torch.Size([4, 512, 4, 4])\n",
            "\n",
            "F(x): LazyConv2d(0, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "output: torch.Size([4, 512, 5, 5])\n",
            "\n",
            "F(x): LazyConv2d(0, 512, kernel_size=(2, 2), stride=(1, 1))\n",
            "output: torch.Size([4, 512, 4, 4])\n",
            "-------------------\n",
            "-------------------\n",
            "ResNet(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            "  (body): Sequential(\n",
            "    (block_2): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (block_3): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (block_4): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (block_5): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "(5073930, 5073930)\n",
            "torch.Size([4, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26VRGYKdsCs0"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils import initialize_parameters, epoch_time"
      ],
      "metadata": {
        "id": "ZRwY48iziDr-"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G8RjsKs0ZjgD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}