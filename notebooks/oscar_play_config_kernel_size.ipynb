{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPB3dA48fr5UdGHdZAuxlJL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ab7289-tandon-nyu/csgy6953_DeepLearning_Midterm/blob/oscar2/notebooks/oscar_test_config_kernel_size.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference: https://www.programiz.com/python-programming/datetime/current-time\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "print(datetime.now(pytz.timezone('America/New_York')).strftime('%m/%d(%a)_%H:%M:%S'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfovHN-sl04P",
        "outputId": "762ce9e5-7596-46a3-c5bc-3d06c960a90b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/18(Fri)_14:38:10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb Install, Login, Import"
      ],
      "metadata": {
        "id": "JEg6gGl4gdFv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrgALvYPgMBy",
        "outputId": "2977d30a-3e51-42c0-bc0f-fd4bc9c3a87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.5)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.11)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login \"6f19b1e6735ebc69af24f18d5b426262416027fb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBNArx2fgf-v",
        "outputId": "16cf5853-374d-42f9-99aa-860c7e224aa0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "MP8Tcl1ogjVP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torchsummary Install, Import"
      ],
      "metadata": {
        "id": "eFOQTotOdDmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-summary==1.4.5"
      ],
      "metadata": {
        "id": "293u_6mjdMDE",
        "outputId": "9b018ce3-b4fe-4108-b000-85b5f50e1a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-summary==1.4.5\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary"
      ],
      "metadata": {
        "id": "h8mq4tcWdOSP",
        "outputId": "b1542b39-5e22-4adf-c000-0765a3d72537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torchsummary.torchsummary.summary(model: torch.nn.modules.module.Module, input_data: Union[torch.Tensor, torch.Size, Sequence[torch.Tensor], Sequence[Union[int, Sequence[Any], torch.Size]], NoneType] = None, *args: Any, batch_dim: Union[int, NoneType] = 0, branching: bool = True, col_names: Union[Iterable[str], NoneType] = None, col_width: int = 25, depth: int = 3, device: Union[torch.device, NoneType] = None, dtypes: Union[List[torch.dtype], NoneType] = None, verbose: int = 1, **kwargs: Any) -> torchsummary.model_statistics.ModelStatistics>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone Team's Code"
      ],
      "metadata": {
        "id": "1bbnVuzegpXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/csgy6953_DeepLearning_Midterm/"
      ],
      "metadata": {
        "id": "rSPl_TQsiRgl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b config_kernel_size_on_updated_main \"https://github.com/ab7289-tandon-nyu/csgy6953_DeepLearning_Midterm.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw2LzDqIgoQt",
        "outputId": "5edaf1f7-393b-4909-bf3f-6a1f4d2cb6b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'csgy6953_DeepLearning_Midterm'...\n",
            "remote: Enumerating objects: 614, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 614 (delta 42), reused 39 (delta 31), pack-reused 507\u001b[K\n",
            "Receiving objects: 100% (614/614), 157.10 KiB | 1.45 MiB/s, done.\n",
            "Resolving deltas: 100% (393/393), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/csgy6953_DeepLearning_Midterm/src/ ."
      ],
      "metadata": {
        "id": "17_B3oo1glfe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat src/model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuMWXJYciM-S",
        "outputId": "1bca47af-3c4e-4ce0-efae-849f66f446bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from enum import Enum\n",
            "from typing import List, Optional, Tuple\n",
            "\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "\n",
            "class ResidualBlockType(Enum):\n",
            "    \"\"\"\n",
            "    Enum class to represent the residual block type for ResNet\n",
            "    \"\"\"\n",
            "\n",
            "    BASIC = 0\n",
            "    BOTTLENECK = 1\n",
            "\n",
            "\n",
            "class LayerType(Enum):\n",
            "    \"\"\"\n",
            "    Enum class to represent layer within for ResidualBlock and for BottleneckResidualBlock\n",
            "    \"\"\"\n",
            "\n",
            "    # Disambiguation: here \"layer\" refers to the individual layer within a block,\n",
            "    # not a \"residual layer\" containing one or more blocks\n",
            "    CONV = 0\n",
            "\n",
            "\n",
            "class LayerLoc(Enum):\n",
            "    \"\"\"\n",
            "    Enum class to represent a layer's location within a block\n",
            "    \"\"\"\n",
            "\n",
            "    MAIN_BLOCK_CONV1 = 0\n",
            "    MAIN_BLOCK_CONV2 = 1\n",
            "    MAIN_BLOCK_CONV3 = 2\n",
            "\n",
            "    SHORTCUT_IDENTITY = 6   # identity\n",
            "    SHORTCUT_CONV_STEM = 7\n",
            "\n",
            "\n",
            "def generate_layer(\n",
            "    block_type: ResidualBlockType,\n",
            "    layer_type: LayerType, \n",
            "    layer_loc: LayerLoc, # position of this layer within the block starting from index 1 \n",
            "    num_channels: int,\n",
            "    main_block_kernel_size: int,\n",
            "    strides: int = 1,\n",
            "    factor: int = 4,\n",
            "    use_bias: bool = False,\n",
            "):\n",
            "    \"\"\"\n",
            "    Returns a layer with the most appropriate parameters such as padding \n",
            "    \"\"\"\n",
            "    if block_type == ResidualBlockType.BASIC:\n",
            "        if layer_type == LayerType.CONV:\n",
            "\n",
            "            if main_block_kernel_size == 3:\n",
            "                layer_locator = {\n",
            "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
            "                        num_channels,\n",
            "                        kernel_size=3, padding=1, # ResidualBlock.conv1\n",
            "                        stride=strides, bias=use_bias\n",
            "                        ), \n",
            "                     LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
            "                        num_channels,\n",
            "                        kernel_size=3, padding=1, # ResidualBlock.conv2\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                     LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "                     LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
            "                        bias=use_bias\n",
            "                        )\n",
            "                    }\n",
            "            \n",
            "            # partial solution 1:\n",
            "            # fatal issue: when input is [256,1,1], MUST pad in order to apply 2x2 kernel\n",
            "            if main_block_kernel_size == 2:\n",
            "                layer_locator = {\n",
            "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
            "                        num_channels,\n",
            "                        kernel_size=2, padding=1, # ResidualBlock.conv1\n",
            "                        stride=strides, bias=use_bias\n",
            "                        ), \n",
            "                     LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
            "                        num_channels,\n",
            "                        kernel_size=2, padding=0, # ResidualBlock.conv2\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                     LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "                     LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
            "                        bias=use_bias\n",
            "                        )\n",
            "                    }\n",
            "            \n",
            "    if block_type == ResidualBlockType.BOTTLENECK:\n",
            "        if layer_type == LayerType.CONV:\n",
            "\n",
            "            if main_block_kernel_size == 3:\n",
            "                layer_locator = {\n",
            "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
            "                        num_channels // factor,\n",
            "                        kernel_size=1, padding=0, # BottleneckResidualBlock.conv1\n",
            "                        bias=use_bias\n",
            "                        ), \n",
            "                    LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
            "                        num_channels // factor,\n",
            "                        kernel_size=3, padding=1, stride=strides, # BottleneckResidualBlock.conv2\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                    LayerLoc.MAIN_BLOCK_CONV3: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, padding=0, # BottleneckResidualBlock.conv3\n",
            "                        bias=use_bias\n",
            "                        ),\n",
            "                    LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
            "                    LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
            "                        num_channels, \n",
            "                        kernel_size=1, stride=strides, # BottleneckResidualBlock.conv_stem\n",
            "                        bias=use_bias\n",
            "                        )\n",
            "                    }\n",
            "    return layer_locator[layer_loc]\n",
            "    \n",
            "\n",
            "class ResidualBlock(nn.Module):\n",
            "    \"\"\"\n",
            "    Class representing a convolutional residual block\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        num_channels: int,\n",
            "        use_stem: bool = False,\n",
            "        strides: int = 1,\n",
            "        dropout: Optional[float] = None,\n",
            "        use_bias: bool = False,\n",
            "        main_block_kernel_size: int = 3\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Creates a new instance of a Residual Block\n",
            "        @param: num_channels (int) - the number of output channels for all convolutions in\n",
            "            the block\n",
            "        @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the\n",
            "            residual\n",
            "        @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
            "        @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.num_channels = num_channels\n",
            "        self.use_stem = use_stem\n",
            "        self.strides = strides\n",
            "\n",
            "        self.dropout = nn.Dropout(dropout) if dropout is not None else None\n",
            "        self.conv1 = generate_layer(\n",
            "            block_type = ResidualBlockType.BASIC,\n",
            "            layer_type = LayerType.CONV,\n",
            "            layer_loc = LayerLoc.MAIN_BLOCK_CONV1,\n",
            "            num_channels = num_channels,\n",
            "            main_block_kernel_size = main_block_kernel_size,\n",
            "            strides = strides,\n",
            "            use_bias = use_bias,\n",
            "        )\n",
            "        self.conv2 = generate_layer(\n",
            "            lock_type = ResidualBlockType.BASIC,\n",
            "            layer_type = LayerType.CONV,\n",
            "            layer_loc = LayerLoc.MAIN_BLOCK_CONV2,\n",
            "            num_channels = num_channels,\n",
            "            main_block_kernel_size = main_block_kernel_size,\n",
            "            use_bias = use_bias,\n",
            "        )\n",
            "        self.relu = nn.ReLU(inplace=True)\n",
            "        self.out = nn.ReLU(inplace=True)\n",
            "        self.bn1 = nn.LazyBatchNorm2d()\n",
            "        self.bn2 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        self.identity = None\n",
            "        self.conv_stem = None\n",
            "        if use_stem:\n",
            "            self.conv_stem = generate_layer(\n",
            "                block_type = ResidualBlockType.BASIC,\n",
            "                layer_type = LayerType.CONV,\n",
            "                layer_loc = LayerLoc.SHORTCUT_CONV_STEM,\n",
            "                num_channels = num_channels, \n",
            "                main_block_kernel_size = main_block_kernel_size,\n",
            "                strides = strides,\n",
            "                use_bias = use_bias\n",
            "            )\n",
            "        else:\n",
            "            self.identity = generate_layer(\n",
            "                block_type = ResidualBlockType.BASIC,\n",
            "                layer_type = LayerType.CONV,\n",
            "                layer_loc = LayerLoc.SHORTCUT_IDENTITY,\n",
            "            )\n",
            "\n",
            "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
            "        shortcut = inputs\n",
            "\n",
            "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
            "        if self.dropout is not None:\n",
            "            x = self.dropout(x)\n",
            "        x = self.bn2(self.conv2(x))\n",
            "        \n",
            "        if self.use_stem:\n",
            "            # downsample skip connection\n",
            "            shortcut = self.conv_stem(shortcut)\n",
            "        else:\n",
            "            shortcut = self.identity(shortcut)\n",
            "\n",
            "        # add in skip connection\n",
            "        x += shortcut\n",
            "        return self.out(x)\n",
            "\n",
            "\n",
            "class BottleneckResidualBlock(nn.Module):\n",
            "    \"\"\"\n",
            "    Class representing a convolutional residual block with a bottleneck\n",
            "    This class was built with reference to:\n",
            "    https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        num_channels: int,\n",
            "        use_stem: bool = False,\n",
            "        strides: int = 1,\n",
            "        factor: int = 4,\n",
            "        dropout: Optional[float] = None,\n",
            "        use_bias: bool = False,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Creates a new instance of a Residual BottleNeck Block\n",
            "        @param: num_channels (int) - the number of output channels for all convolutions in the block\n",
            "        @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the residual\n",
            "        @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
            "        @param: factor (int) - the factor by which the input channels will be reduced for the bottleneck\n",
            "        @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.num_channels = num_channels\n",
            "        self.use_stem = use_stem\n",
            "        self.strides = strides\n",
            "        self.factor = factor\n",
            "        self.dropout1 = nn.Dropout(dropout) if dropout is not None else None\n",
            "        self.dropout2 = nn.Dropout(dropout) if dropout is not None else None\n",
            "\n",
            "        # First convolutional layer with normalization\n",
            "        self.conv1 = nn.LazyConv2d(\n",
            "            num_channels // factor, kernel_size=1, padding=0, bias=use_bias\n",
            "        )\n",
            "        self.bn1 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        # Second convolutional layer with normalization\n",
            "        self.conv2 = nn.LazyConv2d(\n",
            "            num_channels // factor,\n",
            "            kernel_size=3,\n",
            "            padding=1,\n",
            "            stride=strides,\n",
            "            bias=use_bias,\n",
            "        )\n",
            "        self.bn2 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        # Third convolutional layer with normalization\n",
            "        self.conv3 = nn.LazyConv2d(\n",
            "            num_channels, kernel_size=1, padding=0, bias=use_bias\n",
            "        )\n",
            "        self.bn3 = nn.LazyBatchNorm2d()\n",
            "\n",
            "        self.relu = nn.ReLU(inplace=True)\n",
            "\n",
            "        self.conv_stem = None\n",
            "        if use_stem:\n",
            "            # Bottleneck residual block\n",
            "            self.conv_stem = nn.LazyConv2d(\n",
            "                num_channels, kernel_size=1, stride=strides, bias=use_bias\n",
            "            )\n",
            "\n",
            "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
            "        shortcut = inputs\n",
            "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
            "        if self.dropout1 is not None:\n",
            "            x = self.dropout1(x)\n",
            "        x = self.relu(self.bn2(self.conv2(x)))\n",
            "        if self.dropout2 is not None:\n",
            "            x = self.dropout2(x)\n",
            "        x = self.bn3(self.conv3(x))\n",
            "        if self.use_stem:\n",
            "            # downsample skip connection\n",
            "            shortcut = self.conv_stem(shortcut)\n",
            "\n",
            "        # add in skip connection\n",
            "        x += shortcut\n",
            "        return self.relu(x)\n",
            "\n",
            "\n",
            "class StemConfig:\n",
            "    \"\"\"\n",
            "    convenience class to encapsulate configuration options\n",
            "    for the ResNet stem\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, num_channels, kernel_size, stride, padding):\n",
            "        self.num_channels = num_channels\n",
            "        self.kernel_size = kernel_size\n",
            "        self.stride = stride\n",
            "        self.padding = padding\n",
            "\n",
            "\n",
            "def generate_block(\n",
            "    block_type: ResidualBlockType,\n",
            "    num_channels: int,\n",
            "    use_stem: bool = False,\n",
            "    strides: int = 1,\n",
            "    factor: int = 4,\n",
            "    dropout: Optional[float] = None,\n",
            "    use_bias: bool = False,\n",
            "):\n",
            "    \"\"\"\n",
            "    Returns either a Residual Block or a ResidualBottleneck\n",
            "    \"\"\"\n",
            "    if block_type == ResidualBlockType.BASIC:\n",
            "        return ResidualBlock(\n",
            "            num_channels,\n",
            "            use_stem=use_stem,\n",
            "            strides=strides,\n",
            "            dropout=dropout,\n",
            "            use_bias=use_bias,\n",
            "        )\n",
            "    else:\n",
            "        return BottleneckResidualBlock(\n",
            "            num_channels,\n",
            "            use_stem=use_stem,\n",
            "            strides=strides,\n",
            "            factor=factor,\n",
            "            dropout=dropout,\n",
            "            use_bias=use_bias,\n",
            "        )\n",
            "\n",
            "\n",
            "class ResNet(nn.Module):\n",
            "    \"\"\"\n",
            "    Class representing a full ResNet model\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        architecture: List[Tuple[ResidualBlockType, int, int, float]],\n",
            "        stem_config: Optional[StemConfig],\n",
            "        output_size: int = 10,\n",
            "        use_bias: bool = False,\n",
            "        *args,\n",
            "        **kwargs,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        returns an instance of a ResNet\n",
            "        \"\"\"\n",
            "        super().__init__()\n",
            "        self.use_bias = use_bias\n",
            "        if stem_config is not None:\n",
            "            self.stem = self.create_stem(\n",
            "                stem_config.num_channels,\n",
            "                stem_config.kernel_size,\n",
            "                stem_config.stride,\n",
            "                stem_config.padding,\n",
            "                use_bias=use_bias,\n",
            "            )\n",
            "        else:\n",
            "            self.stem = self.create_stem(use_bias=use_bias)\n",
            "        self.classifier = self.create_classifier(output_size, use_bias=use_bias)\n",
            "\n",
            "        self.body = nn.Sequential()\n",
            "        for idx, block_def in enumerate(architecture):\n",
            "            self.body.add_module(\n",
            "                f\"block_{idx+2}\",\n",
            "                self.create_block(\n",
            "                    *block_def, first_block=(idx == 0), use_bias=use_bias\n",
            "                ),\n",
            "            )\n",
            "\n",
            "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
            "        \"\"\"\n",
            "        Performs forward pass of the inputs through the network\n",
            "        \"\"\"\n",
            "        x = self.stem(inputs)\n",
            "        x = self.body(x)\n",
            "        return self.classifier(x)\n",
            "\n",
            "    def create_stem(\n",
            "        self,\n",
            "        num_channels: int = 64,\n",
            "        kernel_size: int = 7,\n",
            "        stride: int = 2,\n",
            "        padding: int = 3,\n",
            "        use_bias: bool = False,\n",
            "    ) -> nn.Sequential:\n",
            "        \"\"\"\n",
            "        Creates a sequential stem as the first component of the model\n",
            "        \"\"\"\n",
            "        return nn.Sequential(\n",
            "            nn.LazyConv2d(\n",
            "                num_channels,\n",
            "                kernel_size=kernel_size,\n",
            "                padding=padding,\n",
            "                stride=stride,\n",
            "                bias=use_bias,\n",
            "            ),\n",
            "            nn.LazyBatchNorm2d(),\n",
            "            nn.ReLU(inplace=True),\n",
            "        )\n",
            "\n",
            "    def create_classifier(\n",
            "        self, num_classes: int, use_bias: bool = False\n",
            "    ) -> nn.Sequential:\n",
            "        \"\"\"\n",
            "        Creates a sequential classifier head at the very\n",
            "        \"\"\"\n",
            "        return nn.Sequential(\n",
            "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.LazyLinear(num_classes)\n",
            "        )\n",
            "\n",
            "    def create_block(\n",
            "        self,\n",
            "        block_type: ResidualBlockType,\n",
            "        num_residuals: int,\n",
            "        num_channels: int,\n",
            "        dropout: Optional[float] = None,\n",
            "        first_block: bool = False,\n",
            "        use_bias: bool = False,\n",
            "    ) -> nn.Sequential:\n",
            "        \"\"\"\n",
            "        Given our inputs, generates either a ResidualBlock or ResidualBottleNeck and addes it to our\n",
            "        sequence of layers\n",
            "        \"\"\"\n",
            "        layer = []\n",
            "        for i in range(num_residuals):\n",
            "            if i == 0 and not first_block:\n",
            "                layer.append(\n",
            "                    generate_block(\n",
            "                        block_type,\n",
            "                        num_channels,\n",
            "                        use_stem=True,\n",
            "                        strides=2,\n",
            "                        dropout=dropout,\n",
            "                        use_bias=use_bias,\n",
            "                    )\n",
            "                )\n",
            "            else:\n",
            "                layer.append(\n",
            "                    generate_block(\n",
            "                        block_type, num_channels, dropout=dropout, use_bias=use_bias\n",
            "                    )\n",
            "                )\n",
            "        return nn.Sequential(*layer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import, Seed, Device"
      ],
      "metadata": {
        "id": "geVocFZngwER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import time\n",
        "import random"
      ],
      "metadata": {
        "id": "Yk6T4FzhgvGv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "-QebqUCUgyc-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lsMOOo-gzTu",
        "outputId": "2057285a-a8a4-4da3-c4af-93cc43552519"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "Br5Rha2PhBoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.data import get_transformed_data, make_data_loaders\n",
        "from src.transforms import make_auto_transforms # used to use: make_transforms\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "VALID_RATIO = 0.1\n",
        "\n",
        "train_data, valid_data, test_data = \\\n",
        "get_transformed_data(make_transforms=make_auto_transforms, valid_ratio=VALID_RATIO)\n",
        "\n",
        "train_iter, valid_iter, test_iter = \\\n",
        "make_data_loaders(train_data, valid_data, test_data, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD28XJ_qg1Ee",
        "outputId": "da4b5e49-0dde-45d9-a701-9d2f538ed793"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "9P0KpocliyKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# from typing import List, Tuple, Optional\n",
        "\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "#     '''\n",
        "#     Class representing a convolutional residual block \n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, num_channels: int, use_stem: bool = False, strides: int = 1, dropout: Optional[float] = None, kernel_size: int = 3):\n",
        "#         '''\n",
        "#         Creates a new instance of a Residual Block\n",
        "#         @param: num_channels (int) - the number of output channels for all convolutions in \n",
        "#             the block\n",
        "#         @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the\n",
        "#             residual\n",
        "#         @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
        "#         @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
        "#         '''\n",
        "#         super().__init__()\n",
        "#         self.num_channels = num_channels\n",
        "#         self.use_stem = use_stem\n",
        "#         self.strides = strides\n",
        "\n",
        "#         self.kernel_size = kernel_size\n",
        "\n",
        "#         self.dropout = nn.Dropout(dropout) if dropout is not None else None\n",
        "\n",
        "#         if self.kernel_size == 3:\n",
        "#             self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1, stride=strides)\n",
        "#             self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "\n",
        "#         # partial solution 1:\n",
        "#         if kernel_size == 2:\n",
        "#             self.conv1 = nn.LazyConv2d(num_channels, kernel_size=2, padding=1, stride=strides)\n",
        "#             self.conv2 = nn.LazyConv2d(num_channels, kernel_size=2, padding=0)\n",
        "#             # fatal issue: when input is [256,1,1], MUST pad in order to apply 2x2 kernel\n",
        "\n",
        "#         # # partial solution 2:\n",
        "#         # if self.kernel_size == 2:\n",
        "#         #     self.conv1 = nn.LazyConv2d(num_channels, kernel_size=2, padding=1, stride=strides)\n",
        "#         #     self.conv2 = nn.LazyConv2d(num_channels, kernel_size=2, padding=1)\n",
        "\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.out = nn.ReLU(inplace=True)\n",
        "#         self.bn1 = nn.LazyBatchNorm2d()\n",
        "#         self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "#         self.conv_stem = None\n",
        "#         if use_stem:\n",
        "\n",
        "#             if kernel_size == 3:\n",
        "#                 self.conv_stem = nn.LazyConv2d(num_channels, kernel_size=1, stride=strides)\n",
        "#             if kernel_size == 2:\n",
        "#                 self.conv_stem = nn.LazyConv2d(num_channels, kernel_size=1, stride=strides)\n",
        "\n",
        "#     def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "#         print()\n",
        "#         print('--------ResidualBlock:--------')\n",
        "#         print(f'(use_stem={self.use_stem})')\n",
        "#         print('block input:', inputs.shape)\n",
        "\n",
        "#         shortcut = inputs\n",
        "\n",
        "#         print()\n",
        "#         print('F(x):', self.conv1)\n",
        "#         x = self.relu(self.bn1(self.conv1(inputs)))\n",
        "#         print('output:', x.shape)\n",
        "\n",
        "#         if self.dropout is not None:\n",
        "#             x = self.dropout(x)\n",
        "        \n",
        "#         print()\n",
        "#         print('F(x):', self.conv2)\n",
        "#         x = self.bn2(self.conv2(x))\n",
        "#         print('output:', x.shape)\n",
        "\n",
        "#         if self.use_stem:\n",
        "#             # downsample skip connection\n",
        "#             print()\n",
        "#             print('S(x):', self.conv_stem)\n",
        "#             shortcut = self.conv_stem(shortcut)\n",
        "#             print('output:', shortcut.shape)\n",
        "\n",
        "#         # partial solution 2:\n",
        "#         # if self.kernel_size == 2:\n",
        "#         #     if x.shape[-1] > shortcut.shape[-1]:\n",
        "#         #         x = F.pad(x, pad = (-1, -1, -1, -1))\n",
        "#         #         print('negative padding =>', x.shape)\n",
        "#         #         # fatal error: when F(x)'s output becomes [512,2,2] whiel input was [256,1,1], this would reduce F(x)'s output to [512,0,0]\n",
        " \n",
        "#         # add in skip connection\n",
        "#         x += shortcut\n",
        "\n",
        "#         return self.out(x)\n",
        "\n",
        "\n",
        "# class StemConfig:\n",
        "#     '''\n",
        "#     convenience class to encapsulate configuration options\n",
        "#     for the ResNet stem\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, num_channels, kernel_size, stride, padding):\n",
        "#         self.num_channels = num_channels\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.stride = stride\n",
        "#         self.padding = padding\n",
        "\n",
        "\n",
        "# class ResNet(nn.Module):\n",
        "#     '''\n",
        "#     Class representing a full ResNet model\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self, architecture: List[Tuple[int, int, float]], stem_config: Optional[StemConfig], output_size: int = 10, *args, **kwargs):\n",
        "#         '''\n",
        "#         returns an instance of a ResNet\n",
        "#         '''\n",
        "#         super().__init__()\n",
        "#         if stem_config is not None:\n",
        "#             self.stem = self.create_stem(\n",
        "#                 stem_config.num_channels,\n",
        "#                 stem_config.kernel_size,\n",
        "#                 stem_config.stride,\n",
        "#                 stem_config.padding\n",
        "#             )\n",
        "#         else:\n",
        "#             self.stem = self.create_stem()\n",
        "#         self.classifier = self.create_classifier(output_size)\n",
        "\n",
        "#         self.body = nn.Sequential()\n",
        "#         for idx, block_def in enumerate(architecture):\n",
        "#             self.body.add_module(\n",
        "#                 f\"block_{idx+2}\", self.create_block(*block_def, first_block=(idx == 0)))\n",
        "\n",
        "#     def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "#         \"\"\"\n",
        "#         Performs forward pass of the inputs through the network\n",
        "#         \"\"\"\n",
        "#         x = self.stem(inputs)\n",
        "#         x = self.body(x)\n",
        "#         return self.classifier(x)\n",
        "\n",
        "#     def create_stem(self, num_channels: int = 64, kernel_size: int = 7, stride: int = 2, padding: int = 3) \\\n",
        "#             -> nn.Sequential:\n",
        "#         \"\"\"\n",
        "#         Creates a sequential stem as the first component of the model\n",
        "#         \"\"\"\n",
        "#         return nn.Sequential(\n",
        "#             nn.LazyConv2d(num_channels, kernel_size=kernel_size,\n",
        "#                           padding=padding, stride=stride),\n",
        "#             nn.LazyBatchNorm2d(),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             # nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "#         )\n",
        "\n",
        "#     def create_classifier(self, num_classes: int) -> nn.Sequential:\n",
        "#         '''\n",
        "#         Creates a sequential classifier head at the very \n",
        "#         '''\n",
        "#         return nn.Sequential(\n",
        "#             nn.AdaptiveAvgPool2d(1),\n",
        "#             nn.Flatten(),\n",
        "#             nn.LazyLinear(num_classes)\n",
        "#         )\n",
        "\n",
        "#     def create_block(self, num_residuals: int, num_channels: int, dropout: float, kernel_size: int, first_block: bool = False) -> nn.Sequential:\n",
        "#         layer = []\n",
        "#         for i in range(num_residuals):\n",
        "#             if i == 0 and not first_block:\n",
        "#                 layer.append(ResidualBlock(num_channels, dropout=dropout, kernel_size=kernel_size, use_stem=True, strides=2))\n",
        "#             else:\n",
        "#                 layer.append(ResidualBlock(num_channels, dropout=dropout, kernel_size=kernel_size))\n",
        "#         return nn.Sequential(*layer)\n"
      ],
      "metadata": {
        "id": "HquK6AiLDIXN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from src.model import StemConfig\n",
        "\n",
        "# # 1 tuple == 1 layer\n",
        "# # how many blocks in each layer, out_channels, dropout prob, kernel_size\n",
        "# architecture = [\n",
        "#     (2, 64, 0.5, 2),\n",
        "#     (2, 128, 0.5, 2),\n",
        "#     (2, 256, 0.5, 2),\n",
        "#     (2, 512, 0.5, 2),\n",
        "# ]"
      ],
      "metadata": {
        "id": "KaKBayOZOcea"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ResidualBlockType(Enum):\n",
        "    \"\"\"\n",
        "    Enum class to represent the residual block type for ResNet\n",
        "    \"\"\"\n",
        "\n",
        "    BASIC = 0\n",
        "    BOTTLENECK = 1\n",
        "\n",
        "\n",
        "class LayerType(Enum):\n",
        "    \"\"\"\n",
        "    Enum class to represent layer within for ResidualBlock and for BottleneckResidualBlock\n",
        "    \"\"\"\n",
        "\n",
        "    # Disambiguation: here \"layer\" refers to the individual layer within a block,\n",
        "    # not a \"residual layer\" containing one or more blocks\n",
        "    CONV = 0\n",
        "\n",
        "\n",
        "class LayerLoc(Enum):\n",
        "    \"\"\"\n",
        "    Enum class to represent a layer's location within a block\n",
        "    \"\"\"\n",
        "\n",
        "    MAIN_BLOCK_CONV1 = 0\n",
        "    MAIN_BLOCK_CONV2 = 1\n",
        "    MAIN_BLOCK_CONV3 = 2\n",
        "\n",
        "    SHORTCUT_IDENTITY = 6   # identity\n",
        "    SHORTCUT_CONV_STEM = 7\n",
        "\n",
        "\n",
        "def generate_layer(\n",
        "    block_type: ResidualBlockType,\n",
        "    layer_type: LayerType, \n",
        "    layer_loc: LayerLoc, # position of this layer within the block starting from index 1 \n",
        "    num_channels: int,\n",
        "    main_block_kernel_size: int,\n",
        "    strides: int = 1,\n",
        "    factor: int = 4,\n",
        "    use_bias: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a layer with the most appropriate parameters such as padding \n",
        "    \"\"\"\n",
        "    if block_type == ResidualBlockType.BASIC:\n",
        "        if layer_type == LayerType.CONV:\n",
        "\n",
        "            if main_block_kernel_size == 3:\n",
        "                layer_locator = {\n",
        "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
        "                        num_channels,\n",
        "                        kernel_size=3, padding=1, # ResidualBlock.conv1\n",
        "                        stride=strides, bias=use_bias\n",
        "                        ), \n",
        "                     LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
        "                        num_channels,\n",
        "                        kernel_size=3, padding=1, # ResidualBlock.conv2\n",
        "                        bias=use_bias\n",
        "                        ),\n",
        "                     LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
        "                     LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
        "                        num_channels, \n",
        "                        kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
        "                        bias=use_bias\n",
        "                        )\n",
        "                    }\n",
        "            \n",
        "            # partial solution 1:\n",
        "            # fatal issue: when input is [256,1,1], MUST pad in order to apply 2x2 kernel\n",
        "            if main_block_kernel_size == 2:\n",
        "                layer_locator = {\n",
        "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
        "                        num_channels,\n",
        "                        kernel_size=2, padding=1, # ResidualBlock.conv1\n",
        "                        stride=strides, bias=use_bias\n",
        "                        ), \n",
        "                     LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
        "                        num_channels,\n",
        "                        kernel_size=2, padding=0, # ResidualBlock.conv2\n",
        "                        bias=use_bias\n",
        "                        ),\n",
        "                     LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
        "                     LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
        "                        num_channels, \n",
        "                        kernel_size=1, stride=strides, # ResidualBlock.conv_stem\n",
        "                        bias=use_bias\n",
        "                        )\n",
        "                    }\n",
        "            \n",
        "    if block_type == ResidualBlockType.BOTTLENECK:\n",
        "        if layer_type == LayerType.CONV:\n",
        "\n",
        "            if main_block_kernel_size == 3:\n",
        "                layer_locator = {\n",
        "                    LayerLoc.MAIN_BLOCK_CONV1: nn.LazyConv2d(\n",
        "                        num_channels // factor,\n",
        "                        kernel_size=1, padding=0, # BottleneckResidualBlock.conv1\n",
        "                        bias=use_bias\n",
        "                        ), \n",
        "                    LayerLoc.MAIN_BLOCK_CONV2: nn.LazyConv2d(\n",
        "                        num_channels // factor,\n",
        "                        kernel_size=3, padding=1, stride=strides, # BottleneckResidualBlock.conv2\n",
        "                        bias=use_bias\n",
        "                        ),\n",
        "                    LayerLoc.MAIN_BLOCK_CONV3: nn.LazyConv2d(\n",
        "                        num_channels, \n",
        "                        kernel_size=1, padding=0, # BottleneckResidualBlock.conv3\n",
        "                        bias=use_bias\n",
        "                        ),\n",
        "                    LayerLoc.SHORTCUT_IDENTITY: nn.Identity(),\n",
        "                    LayerLoc.SHORTCUT_CONV_STEM: nn.LazyConv2d(\n",
        "                        num_channels, \n",
        "                        kernel_size=1, stride=strides, # BottleneckResidualBlock.conv_stem\n",
        "                        bias=use_bias\n",
        "                        )\n",
        "                    }\n",
        "    return layer_locator[layer_loc]\n",
        "    \n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Class representing a convolutional residual block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_channels: int,\n",
        "        use_stem: bool = False,\n",
        "        strides: int = 1,\n",
        "        dropout: Optional[float] = None,\n",
        "        use_bias: bool = False,\n",
        "        main_block_kernel_size: int = 3\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Creates a new instance of a Residual Block\n",
        "        @param: num_channels (int) - the number of output channels for all convolutions in\n",
        "            the block\n",
        "        @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the\n",
        "            residual\n",
        "        @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
        "        @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_channels = num_channels\n",
        "        self.use_stem = use_stem\n",
        "        self.strides = strides\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout) if dropout is not None else None\n",
        "        self.conv1 = generate_layer(\n",
        "            block_type = ResidualBlockType.BASIC,\n",
        "            layer_type = LayerType.CONV,\n",
        "            layer_loc = LayerLoc.MAIN_BLOCK_CONV1,\n",
        "            num_channels = num_channels,\n",
        "            main_block_kernel_size = main_block_kernel_size,\n",
        "            strides = strides,\n",
        "            use_bias = use_bias,\n",
        "        )\n",
        "        self.conv2 = generate_layer(\n",
        "            block_type = ResidualBlockType.BASIC,\n",
        "            layer_type = LayerType.CONV,\n",
        "            layer_loc = LayerLoc.MAIN_BLOCK_CONV2,\n",
        "            num_channels = num_channels,\n",
        "            main_block_kernel_size = main_block_kernel_size,\n",
        "            use_bias = use_bias,\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.out = nn.ReLU(inplace=True)\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "        self.identity = None\n",
        "        self.conv_stem = None\n",
        "        if use_stem:\n",
        "            self.conv_stem = generate_layer(\n",
        "                block_type = ResidualBlockType.BASIC,\n",
        "                layer_type = LayerType.CONV,\n",
        "                layer_loc = LayerLoc.SHORTCUT_CONV_STEM,\n",
        "                num_channels = num_channels, \n",
        "                main_block_kernel_size = main_block_kernel_size,\n",
        "                strides = strides,\n",
        "                use_bias = use_bias\n",
        "            )\n",
        "        else:\n",
        "            self.identity = generate_layer(\n",
        "                block_type = ResidualBlockType.BASIC,\n",
        "                layer_type = LayerType.CONV,\n",
        "                layer_loc = LayerLoc.SHORTCUT_IDENTITY,\n",
        "                num_channels = num_channels,\n",
        "                main_block_kernel_size = main_block_kernel_size\n",
        "            )\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        shortcut = inputs\n",
        "\n",
        "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        \n",
        "        if self.use_stem:\n",
        "            # downsample skip connection\n",
        "            shortcut = self.conv_stem(shortcut)\n",
        "        else:\n",
        "            shortcut = self.identity(shortcut)\n",
        "\n",
        "        # add in skip connection\n",
        "        x += shortcut\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "class BottleneckResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Class representing a convolutional residual block with a bottleneck\n",
        "    This class was built with reference to:\n",
        "    https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_channels: int,\n",
        "        use_stem: bool = False,\n",
        "        strides: int = 1,\n",
        "        factor: int = 4,\n",
        "        dropout: Optional[float] = None,\n",
        "        use_bias: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Creates a new instance of a Residual BottleNeck Block\n",
        "        @param: num_channels (int) - the number of output channels for all convolutions in the block\n",
        "        @param: use_stem (bool) - whether a 1x1 convolution is needed to downsample the residual\n",
        "        @param: strides (int) - the number of strides to use in the convolutions, defaults to 1\n",
        "        @param: factor (int) - the factor by which the input channels will be reduced for the bottleneck\n",
        "        @param: dropout (float) - if present, adds a dropout between the hidden layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_channels = num_channels\n",
        "        self.use_stem = use_stem\n",
        "        self.strides = strides\n",
        "        self.factor = factor\n",
        "        self.dropout1 = nn.Dropout(dropout) if dropout is not None else None\n",
        "        self.dropout2 = nn.Dropout(dropout) if dropout is not None else None\n",
        "\n",
        "        # First convolutional layer with normalization\n",
        "        self.conv1 = nn.LazyConv2d(\n",
        "            num_channels // factor, kernel_size=1, padding=0, bias=use_bias\n",
        "        )\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "\n",
        "        # Second convolutional layer with normalization\n",
        "        self.conv2 = nn.LazyConv2d(\n",
        "            num_channels // factor,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            stride=strides,\n",
        "            bias=use_bias,\n",
        "        )\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "        # Third convolutional layer with normalization\n",
        "        self.conv3 = nn.LazyConv2d(\n",
        "            num_channels, kernel_size=1, padding=0, bias=use_bias\n",
        "        )\n",
        "        self.bn3 = nn.LazyBatchNorm2d()\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv_stem = None\n",
        "        if use_stem:\n",
        "            # Bottleneck residual block\n",
        "            self.conv_stem = nn.LazyConv2d(\n",
        "                num_channels, kernel_size=1, stride=strides, bias=use_bias\n",
        "            )\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        shortcut = inputs\n",
        "        x = self.relu(self.bn1(self.conv1(inputs)))\n",
        "        if self.dropout1 is not None:\n",
        "            x = self.dropout1(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        if self.dropout2 is not None:\n",
        "            x = self.dropout2(x)\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        if self.use_stem:\n",
        "            # downsample skip connection\n",
        "            shortcut = self.conv_stem(shortcut)\n",
        "\n",
        "        # add in skip connection\n",
        "        x += shortcut\n",
        "        return self.relu(x)\n",
        "\n",
        "\n",
        "class StemConfig:\n",
        "    \"\"\"\n",
        "    convenience class to encapsulate configuration options\n",
        "    for the ResNet stem\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels, kernel_size, stride, padding):\n",
        "        self.num_channels = num_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "\n",
        "def generate_block(\n",
        "    block_type: ResidualBlockType,\n",
        "    num_channels: int,\n",
        "    use_stem: bool = False,\n",
        "    strides: int = 1,\n",
        "    factor: int = 4,\n",
        "    dropout: Optional[float] = None,\n",
        "    use_bias: bool = False,\n",
        "    main_block_kernel_size: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns either a Residual Block or a ResidualBottleneck\n",
        "    \"\"\"\n",
        "    if block_type == ResidualBlockType.BASIC:\n",
        "        return ResidualBlock(\n",
        "            num_channels,\n",
        "            use_stem=use_stem,\n",
        "            strides=strides,\n",
        "            dropout=dropout,\n",
        "            use_bias=use_bias,\n",
        "            main_block_kernel_size=main_block_kernel_size\n",
        "        )\n",
        "    else:\n",
        "        return BottleneckResidualBlock(\n",
        "            num_channels,\n",
        "            use_stem=use_stem,\n",
        "            strides=strides,\n",
        "            factor=factor,\n",
        "            dropout=dropout,\n",
        "            use_bias=use_bias,\n",
        "            main_block_kernel_size=main_block_kernel_size\n",
        "        )\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Class representing a full ResNet model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        architecture: List[Tuple[ResidualBlockType, int, int, float, int]],\n",
        "        stem_config: Optional[StemConfig],\n",
        "        output_size: int = 10,\n",
        "        use_bias: bool = False,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        returns an instance of a ResNet\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.use_bias = use_bias\n",
        "        if stem_config is not None:\n",
        "            self.stem = self.create_stem(\n",
        "                stem_config.num_channels,\n",
        "                stem_config.kernel_size,\n",
        "                stem_config.stride,\n",
        "                stem_config.padding,\n",
        "                use_bias=use_bias,\n",
        "            )\n",
        "        else:\n",
        "            self.stem = self.create_stem(use_bias=use_bias)\n",
        "        self.classifier = self.create_classifier(output_size, use_bias=use_bias)\n",
        "\n",
        "        self.body = nn.Sequential()\n",
        "        for idx, block_def in enumerate(architecture):\n",
        "            self.body.add_module(\n",
        "                f\"block_{idx+2}\",\n",
        "                self.create_block(\n",
        "                    *block_def, first_block=(idx == 0), use_bias=use_bias\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs forward pass of the inputs through the network\n",
        "        \"\"\"\n",
        "        x = self.stem(inputs)\n",
        "        x = self.body(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "    def create_stem(\n",
        "        self,\n",
        "        num_channels: int = 64,\n",
        "        kernel_size: int = 7,\n",
        "        stride: int = 2,\n",
        "        padding: int = 3,\n",
        "        use_bias: bool = False,\n",
        "    ) -> nn.Sequential:\n",
        "        \"\"\"\n",
        "        Creates a sequential stem as the first component of the model\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(\n",
        "                num_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                padding=padding,\n",
        "                stride=stride,\n",
        "                bias=use_bias,\n",
        "            ),\n",
        "            nn.LazyBatchNorm2d(),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def create_classifier(\n",
        "        self, num_classes: int, use_bias: bool = False\n",
        "    ) -> nn.Sequential:\n",
        "        \"\"\"\n",
        "        Creates a sequential classifier head at the very\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.LazyLinear(num_classes)\n",
        "        )\n",
        "\n",
        "    def create_block(\n",
        "        self,\n",
        "        block_type: ResidualBlockType,\n",
        "        num_residuals: int,\n",
        "        num_channels: int,\n",
        "        dropout: Optional[float] = None,\n",
        "        main_block_kernel_size: int = 3,\n",
        "        first_block: bool = False,\n",
        "        use_bias: bool = False,\n",
        "    ) -> nn.Sequential:\n",
        "        \"\"\"\n",
        "        Given our inputs, generates either a ResidualBlock or ResidualBottleNeck and addes it to our\n",
        "        sequence of layers\n",
        "        \"\"\"\n",
        "        layer = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                layer.append(\n",
        "                    generate_block(\n",
        "                        block_type,\n",
        "                        num_channels,\n",
        "                        use_stem=True,\n",
        "                        strides=2,\n",
        "                        dropout=dropout,\n",
        "                        use_bias=use_bias,\n",
        "                        main_block_kernel_size=main_block_kernel_size\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                layer.append(\n",
        "                    generate_block(\n",
        "                        block_type, num_channels, dropout=dropout, use_bias=use_bias, \n",
        "                        main_block_kernel_size=main_block_kernel_size\n",
        "                    )\n",
        "                )\n",
        "        return nn.Sequential(*layer)\n"
      ],
      "metadata": {
        "id": "btTVg3PxQV-B"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.model import StemConfig, ResidualBlockType # ResNet\n",
        "\n",
        "MAIN_BLOCK_KERNEL_SIZE = 2\n",
        "\n",
        "# 1 tuple == 1 layer\n",
        "# block with or without bottleneck, how many blocks in each layer, out_channels, dropout prob, kernel_size\n",
        "architecture = [\n",
        "    (ResidualBlockType.BASIC, 2,  64, 0.5, MAIN_BLOCK_KERNEL_SIZE),\n",
        "    (ResidualBlockType.BASIC, 2, 128, 0.5, MAIN_BLOCK_KERNEL_SIZE),\n",
        "    (ResidualBlockType.BASIC, 2, 256, 0.5, MAIN_BLOCK_KERNEL_SIZE),\n",
        "    (ResidualBlockType.BASIC, 2, 512, 0.5, MAIN_BLOCK_KERNEL_SIZE),\n",
        "]"
      ],
      "metadata": {
        "id": "ORrGNYQGix9m"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for temp_idx, temp_block_def in enumerate(architecture):\n",
        "    print(temp_idx, temp_block_def)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-S5-GugSyXX",
        "outputId": "b4530b2a-64ca-48c5-8e23-1dae2094f48d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 (<ResidualBlockType.BASIC: 0>, 2, 64, 0.5, 2)\n",
            "1 (<ResidualBlockType.BASIC: 0>, 2, 128, 0.5, 2)\n",
            "2 (<ResidualBlockType.BASIC: 0>, 2, 256, 0.5, 2)\n",
            "3 (<ResidualBlockType.BASIC: 0>, 2, 512, 0.5, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = StemConfig(num_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "model  = ResNet(architecture, stem_config=config, output_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz6S1KmzOjda",
        "outputId": "a88763db-4673-4720-8167-92d9299483a8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils import count_parameters"
      ],
      "metadata": {
        "id": "HtLNSJIGUH5K"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize model shapes"
      ],
      "metadata": {
        "id": "7LwVNRxIUnAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# intialize a new model\n",
        "\n",
        "# inputs = torch.empty((BATCH_SIZE, 3, 512, 512)) # passed\n",
        "inputs = torch.empty((BATCH_SIZE, 3, 32, 32)) #  passed\n",
        "# inputs = torch.empty((BATCH_SIZE, 3, 4, 4))\n",
        "\n",
        "inputs.normal_()\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "outputs = model(inputs.to(device)) \n",
        "# (Oscar) observation: 2022.11/13(7)_a08.14: internally, this converts all nn.LazyConv2d layers to nn.Conv2d\n",
        "# to see this, run print(model) both before and after this operation\n",
        "\n",
        "print('-------------------')\n",
        "print('-------------------')\n",
        "\n",
        "print(model)\n",
        "print(count_parameters(model))\n",
        "print(outputs.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Dw4mJFmkKu",
        "outputId": "fcf52fad-438d-45e2-dd04-45d826099fbf"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------\n",
            "-------------------\n",
            "ResNet(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=1)\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            "  (body): Sequential(\n",
            "    (block_2): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "    (block_3): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "    (block_4): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "    (block_5): Sequential(\n",
            "      (0): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv_stem): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      )\n",
            "      (1): ResidualBlock(\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (out): ReLU(inplace=True)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (identity): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "(5069130, 5069130)\n",
            "torch.Size([128, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count parameters"
      ],
      "metadata": {
        "id": "eWvfEz42UfD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_parameters, num_parameters_requiring_grad = count_parameters(model)\n",
        "print(num_parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJeHP8QWT_mQ",
        "outputId": "bf3b109b-f19e-4a21-bc59-f2f0acfbd077"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5069130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUIET_VERBOSE = 0\n",
        "\n",
        "one_sample_input_shape = (3, 32, 32)\n",
        "\n",
        "summary(model, one_sample_input_shape, verbose = QUIET_VERBOSE)"
      ],
      "metadata": {
        "id": "yuhd85NQdSVI",
        "outputId": "0f48182c-0c48-47d4-ec31-8ab3b17b4fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─Sequential: 1-1                        [-1, 64, 32, 32]          --\n",
              "|    └─Conv2d: 2-1                       [-1, 64, 32, 32]          1,728\n",
              "|    └─BatchNorm2d: 2-2                  [-1, 64, 32, 32]          128\n",
              "|    └─ReLU: 2-3                         [-1, 64, 32, 32]          --\n",
              "├─Sequential: 1-2                        [-1, 512, 4, 4]           --\n",
              "|    └─Sequential: 2-4                   [-1, 64, 32, 32]          --\n",
              "|    |    └─ResidualBlock: 3-1           [-1, 64, 32, 32]          33,024\n",
              "|    |    └─ResidualBlock: 3-2           [-1, 64, 32, 32]          33,024\n",
              "|    └─Sequential: 2-5                   [-1, 128, 16, 16]         --\n",
              "|    |    └─ResidualBlock: 3-3           [-1, 128, 16, 16]         107,008\n",
              "|    |    └─ResidualBlock: 3-4           [-1, 128, 16, 16]         131,584\n",
              "|    └─Sequential: 2-6                   [-1, 256, 8, 8]           --\n",
              "|    |    └─ResidualBlock: 3-5           [-1, 256, 8, 8]           427,008\n",
              "|    |    └─ResidualBlock: 3-6           [-1, 256, 8, 8]           525,312\n",
              "|    └─Sequential: 2-7                   [-1, 512, 4, 4]           --\n",
              "|    |    └─ResidualBlock: 3-7           [-1, 512, 4, 4]           1,705,984\n",
              "|    |    └─ResidualBlock: 3-8           [-1, 512, 4, 4]           2,099,200\n",
              "├─Sequential: 1-3                        [-1, 10]                  --\n",
              "|    └─AdaptiveAvgPool2d: 2-8            [-1, 512, 1, 1]           --\n",
              "|    └─Flatten: 2-9                      [-1, 512]                 --\n",
              "|    └─Linear: 2-10                      [-1, 10]                  5,130\n",
              "==========================================================================================\n",
              "Total params: 5,069,130\n",
              "Trainable params: 5,069,130\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 292.74\n",
              "==========================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 9.47\n",
              "Params size (MB): 19.34\n",
              "Estimated Total Size (MB): 28.82\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize parameters"
      ],
      "metadata": {
        "id": "JQKBIE8_UgoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils import initialize_parameters\n",
        "\n",
        "# initialize model weights\n",
        "model.apply(initialize_parameters);"
      ],
      "metadata": {
        "id": "uXBP9Cm0Up-h"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train, validate, log"
      ],
      "metadata": {
        "id": "7xAeC3JkTZPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.engine import train_one_epoch, evaluate"
      ],
      "metadata": {
        "id": "G8RjsKs0ZjgD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS  = 10\n",
        "\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "KDl8ESMqTroa"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "ssSEoTTzTvBI"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## setup wandb logging\n",
        "\n",
        "assert 'wandb' in locals() # 'wandb' has been imported already\n",
        "\n",
        "ENTITY_NAME  = 'dlf22_mini_project' # this is our team name\n",
        "PROJECT_NAME = 'Junk_Test_Project'\n",
        "RUN_NAME     = 'resnet_osca_kernel_size_2_2022.11.18.p02.59'\n",
        "\n",
        "WANDB_CONFIG = \\\n",
        "{\"architecture\"  : architecture,    # the model\n",
        " \"num_parameters\": num_parameters,  # the model\n",
        " \"learning_rate\" : learning_rate,   # how to gradient descent\n",
        " \"batch_size\"    : BATCH_SIZE,      # how to gradient descent\n",
        " \"epochs\"        : EPOCHS,}         # train for how long\n",
        "\n",
        "wandb\\\n",
        ".init(name=RUN_NAME, project=PROJECT_NAME, entity=ENTITY_NAME,config=WANDB_CONFIG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "VwTSlzxwTwLf",
        "outputId": "1ea3c3e8-51ae-4a73-b6dc-2c1e22f4cb78"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2qtyvm71) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">resnet_osca_kernel_size_2_2022.11.18.p02.59</strong>: <a href=\"https://wandb.ai/dlf22_mini_project/Junk_Test_Project/runs/2qtyvm71\" target=\"_blank\">https://wandb.ai/dlf22_mini_project/Junk_Test_Project/runs/2qtyvm71</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221118_195937-2qtyvm71/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2qtyvm71). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221118_200243-1za84e4p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/dlf22_mini_project/Junk_Test_Project/runs/1za84e4p\" target=\"_blank\">resnet_osca_kernel_size_2_2022.11.18.p02.59</a></strong> to <a href=\"https://wandb.ai/dlf22_mini_project/Junk_Test_Project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dlf22_mini_project/Junk_Test_Project/runs/1za84e4p?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f569bc8eb50>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save to disk the \"best\" model === the model with the lowest validation loss\n",
        "best_model_path = RUN_NAME + '.pt'"
      ],
      "metadata": {
        "id": "nMyGRL1GUGXH"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils import epoch_time\n",
        "\n",
        "best_loss = float('inf')\n",
        "for epoch in range(11, EPOCHS+10+1):\n",
        "\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    \n",
        "    ## TRAIN\n",
        "    start = time.time()\n",
        "    # train\n",
        "    train_loss, train_acc  = train_one_epoch(model, train_iter, criterion, optimizer, device)\n",
        "    # log & echo\n",
        "    train_mins, train_secs = epoch_time(start, time.time())\n",
        "    wandb.log({\"train_loss\": train_loss,\n",
        "               \"train_acc\" : train_acc,\n",
        "               \"epoch\"     : epoch})\n",
        "    print(f\"\\tTrain elapsed: {train_mins}:{train_secs}, loss: {train_loss:.4f}, acc: {train_acc * 100:.2f}%\")\n",
        "\n",
        "    ## VALIDATE\n",
        "    start = time.time()\n",
        "    # validate\n",
        "    val_loss, val_acc  = evaluate(model, valid_iter, criterion, device)\n",
        "    # log & echo   \n",
        "    val_mins, val_secs = epoch_time(start, time.time())\n",
        "    wandb.log({\"val_loss\": val_loss,\n",
        "               \"val_acc\" : val_acc,\n",
        "               \"epoch\"   : epoch,})\n",
        "    print(f\"\\tValidation elapsed: {val_mins}:{val_secs}, loss: {val_loss:.4f}, acc: {val_acc * 100:.2f}%\")\n",
        "\n",
        "    ## Memorize \"best\" model === the model with the lowest validation loss\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWjFOiMULJ3",
        "outputId": "7a24b9cb-06a6-468c-fe23-fac31086cd95"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11\n",
            "\tTrain elapsed: 0:59, loss: 0.7599, acc: 73.29%\n",
            "\tValidation elapsed: 0:1, loss: 0.5800, acc: 80.51%\n",
            "Epoch 12\n",
            "\tTrain elapsed: 1:0, loss: 0.7153, acc: 75.08%\n",
            "\tValidation elapsed: 0:1, loss: 0.5992, acc: 80.10%\n",
            "Epoch 13\n",
            "\tTrain elapsed: 1:0, loss: 0.6907, acc: 75.89%\n",
            "\tValidation elapsed: 0:1, loss: 0.5318, acc: 82.19%\n",
            "Epoch 14\n",
            "\tTrain elapsed: 1:0, loss: 0.6645, acc: 76.69%\n",
            "\tValidation elapsed: 0:1, loss: 0.5305, acc: 82.29%\n",
            "Epoch 15\n",
            "\tTrain elapsed: 1:0, loss: 0.6330, acc: 77.98%\n",
            "\tValidation elapsed: 0:1, loss: 0.5245, acc: 82.05%\n",
            "Epoch 16\n",
            "\tTrain elapsed: 1:0, loss: 0.6102, acc: 78.62%\n",
            "\tValidation elapsed: 0:1, loss: 0.4962, acc: 83.87%\n",
            "Epoch 17\n",
            "\tTrain elapsed: 0:59, loss: 0.5885, acc: 79.42%\n",
            "\tValidation elapsed: 0:1, loss: 0.4883, acc: 83.75%\n",
            "Epoch 18\n",
            "\tTrain elapsed: 0:59, loss: 0.5665, acc: 80.21%\n",
            "\tValidation elapsed: 0:1, loss: 0.5026, acc: 83.79%\n",
            "Epoch 19\n",
            "\tTrain elapsed: 1:0, loss: 0.5480, acc: 80.81%\n",
            "\tValidation elapsed: 0:1, loss: 0.4819, acc: 84.36%\n",
            "Epoch 20\n",
            "\tTrain elapsed: 1:0, loss: 0.5371, acc: 81.30%\n",
            "\tValidation elapsed: 0:1, loss: 0.4779, acc: 84.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p92FzUqGUMgP"
      },
      "execution_count": 78,
      "outputs": []
    }
  ]
}